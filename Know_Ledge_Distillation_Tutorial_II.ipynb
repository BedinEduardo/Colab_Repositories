{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgOHH0HEbDwkLCAuJvwEcf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BedinEduardo/Colab_Repositories/blob/master/Know_Ledge_Distillation_Tutorial_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distilation Tutorial\n",
        "\n",
        "Knowledge Distilation is a technique that enebles knowledge transfer from large, computationally expensive models to smaller ones without losing validity.\n",
        "This allows for deployment on less powwerfull hardware, making evaluation faster and more efficient.\n",
        "\n",
        "In this tutorial, we will run a number of experiments focused at improving the accuracy of a lightweight NN, using a more powerful network as a teacher.\n",
        "The computational costa and the speed of the lightweight network will remain unaffected, our intervention only focuses on its weights, not on its forward pass.\n",
        "\n",
        "Will learn:\n",
        "* How to modify model classes to extract hidden representations and use them for futher calculations.\n",
        "* How to modify regular train loops in PyTorch to include additional losses on top, for example, cross-entropy for classification\n",
        "* How to improve the performance of lightweith models by using more complex models as teacher.\n",
        "\n",
        "## Prerequesites\n",
        "\n",
        "* 1 GPU, 4GB of memory\n",
        "* PyTorch V2.0 or smaller\n",
        "* CIFAR-10 datasets"
      ],
      "metadata": {
        "id": "funSVT5S7QHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "dV-fkiRMTwA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "vo6AzAcKT-Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNAXEmTqUNhR",
        "outputId": "6545684e-8a7c-4af2-933d-7b86ad75c179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading CIFAR10\n",
        "\n",
        "Cifar is a popular image dataset with ten classes.\n",
        "The tutorial objective is to predict one of the following classes for each input image.\n",
        "\n",
        "The input image are RGB, so they have 3 channels and are 32 x 32 pixels.\n",
        "3 x 32 x 32 = 3072 pixesl --> number ranging from 0 to 255.\n",
        "Commom practice in NN is to normalize the input data\n",
        "--> Avoid saturation in commonly activation functions --> increase numerical stability.\n",
        "The current normalization proccess consist of subtracting the mean and dividing by the standard deviation along each channel.\n",
        "The tensor “mean=[0.485, 0.456, 0.406]” and “std=[0.229, 0.224, 0.225]\" were already computaed, they represent the mean and standard deviation of each channel --> in the predefined subset of CIFAR-10 --> training set.\n",
        "--> The values for test set as well --> The NN is trained on features produced by subtracting and dividing the numbers above --> maintain the consistency.\n",
        "In real world, wew are not able to compute the mean and the standard deviation."
      ],
      "metadata": {
        "id": "q3J2BoezUOk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data for CIFAR-10 - arbitary batch size of 128\n",
        "transforms_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),  # transfrom to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "fgpX31kbWLey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the CIFAR-10 dataset:\n",
        "training_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "testing_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi53l-N1WqRr",
        "outputId": "673dc015-f2ef-4357-c558-6fdcb4f8bf59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now defining the DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "QGZKLbwlXCEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining model classes and utility functions\n",
        "\n",
        "Next, we need to define our model classes.\n",
        "Several user-defined parameters need to be set here.\n",
        "Use two different architectures, keeping the number of the filters fixed accros our experiments to ensure fair comparisions.\n",
        "Both are CNN --> With different number of Conv layers --> serve as feature extractors --> Followed by a classifier with 10 classes.\n",
        "\n",
        "The number of filters and neurons is smaller for the students\n",
        "\n",
        "In this point get how to do this in these examples, and after this two steps:\n",
        "\n",
        "1. Replicate the codes in a VS in run it in the notebook and after in workstations.\n",
        "2. Study an understand how to adapt others models, larger models and smaller models and try do the same steps of the code\n",
        "3. Replicate it in VS code an run in the notebook and workstation.\n",
        "4. Study other tutorials and run the distilation for detection\n",
        "5. And replicate the code in VS code."
      ],
      "metadata": {
        "id": "WsFVwuZsXTE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deeper NN class to be used as a teacher:\n",
        "class DeepNN(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(DeepNN, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3, 128, kernel_size=3, padding=1),  # nn.Conv2d(3, ..., ...) the three is the number of channels, kernel size --> how much pixels per filter, padding --> how much moves in each iterations\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128,64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, padding=1),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,32, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )  # this block the Conv layers extract features\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(2048,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x,1)  # flattenize the tensor\n",
        "    x = self.classifier(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "zaE3tJGvZp-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = DeepNN(num_classes=10).to(device)\n",
        "print(teacher)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F457hxnnTZZ",
        "outputId": "a5f45f3b-f49f-43c0-9bdc-bc7dd9a5ed5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU()\n",
            "    (7): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU()\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Light weight model -- Student model\n",
        "class LightNN(nn.Module):\n",
        "  def __init__(self, num_classes=10): # must have the same output shape -- number of classes\n",
        "    super(LightNN, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3,16, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(16,16, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(1024, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(256, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "hJCRAbSsnZdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student = LightNN(num_classes=10).to(device)\n",
        "print(student)"
      ],
      "metadata": {
        "id": "dfxDVNQ4gvru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a23c9b-b808-4131-8102-5e864c6defce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Waas employed 2 functions to help produces and evaluate the resulsts --> classification tasks\n",
        "* **model**: A model instance to train - update weights -- via this function\n",
        "* **train_loader**: we define **train_loader**, and its job is to feed the data into the model\n",
        "* **epochs**: how many times we loop over the dataset\n",
        "* **Learning_rate**: the learning rate determines how large our steps towards convergence should be.\n",
        "* **device**: determines the device to run the workload --> CPU, GPU, or other hardware accelerator\n",
        "\n",
        "The test function is similar --> but it will invoked **test_loader** to load imates from the test set\n",
        "\n",
        "### Steps to train, and transfer learning from teacher to student\n",
        "1. Train the teacher and the student of the same training dataset, and evaluate in the same test set.\n",
        "2. After the teacher to be finetunned, start the KD process\n",
        "3. The teacher goes to evaluation model, and the student stay in the train mode\n",
        "4. Forward pass a data (image) into the student (in train mode) and the same data into the teacher (in evaluation mode)\n",
        "5. Get the output probabilities (logits) of the student and compare with the teacher\n",
        "6. The KD is doing adjusting the weights and biases of the student by a comparision between:\n",
        "  * The loss of teacher and student, or;\n",
        "  * The features of the teachers and student, or;\n",
        "  * The logits of the teachers and student,"
      ],
      "metadata": {
        "id": "Kp3ddA2oqiUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before to continue we should dvelop a train and validation functions - to normal train and validation\n",
        "def train(model, train_loader, epochs, learning_rate, device):  # here can also be inserted the LR_decay\n",
        "  loss_fn = nn.CrossEntropyLoss()  # loss for classification - it can be seted from  hyperparameters\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  model.train() # setting the model to train mode\n",
        "\n",
        "  for epoch in range(epochs):  # after peerform this example and the detection exemples -- adapt the code to train  for KD Detection\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "      # inputs: A collection of batch_size mages\n",
        "      # labels:: a vector of dimensionality batch_size with integers denoting class of each image\n",
        "      inputs, labels = inputs.to(device), labels.to(device)  # sending to device\n",
        "\n",
        "      optimizer.zero_grad()  # zeroing the gradients\n",
        "      outputs = model(inputs)\n",
        "      # outputs: output of the NN for the collection images. A tensor with dimensionality batch_size x num_classes\n",
        "      # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "      loss = loss_fn(outputs, labels)  # compare the predicted with the ground truth\n",
        "      loss.backward()  # adjust the network based on the loss calculated before\n",
        "      optimizer.step()  # adjust the gradients based on the loss\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "id": "rf-i1Bj0tzfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device):  # adjust the code to perform cross validation\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():  # for validation step did not use the gradients graph activate\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)   # get the logits - the maximum values\n",
        "\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item() # sum and itemize if the predicted value is equal the current label\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f\"Test acc: {accuracy:.3f}\")\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "X79hc3ZuRINi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Entropy runs\n",
        "\n",
        "Start by training the teacher network using cross-entropy:\n"
      ],
      "metadata": {
        "id": "BGoIcBAmSTvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now train the teacher, as the 1st step\n",
        "train(model=teacher,\n",
        "      train_loader=train_loader,\n",
        "      epochs=11,\n",
        "      learning_rate=0.001,\n",
        "      device=device)\n",
        "test_accuracy_teacher = test(model=teacher,\n",
        "                             test_loader=test_loader,\n",
        "                             device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ2sLV5YSq60",
        "outputId": "dd081c6c-dd7c-45c0-9bd4-4abacff9aa43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11, Loss: 1.3355022575849158\n",
            "Epoch 2/11, Loss: 0.8750486335409876\n",
            "Epoch 3/11, Loss: 0.6935216221586823\n",
            "Epoch 4/11, Loss: 0.5611832777938575\n",
            "Epoch 5/11, Loss: 0.45006790917242884\n",
            "Epoch 6/11, Loss: 0.3500845811098738\n",
            "Epoch 7/11, Loss: 0.26924474640270635\n",
            "Epoch 8/11, Loss: 0.21906665719740684\n",
            "Epoch 9/11, Loss: 0.18478701465174824\n",
            "Epoch 10/11, Loss: 0.16069171736802895\n",
            "Epoch 11/11, Loss: 0.14771360345899373\n",
            "Test acc: 97.508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the one more Student Network to compare their performance -- Back propagation is sensitive to weight initialization, so we need to make sure that\n",
        "# the two NN has the same initialization\n",
        "new_student = LightNN(num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "JoIbR7R5TRvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure we have builded a copy of the 1st network, we inspect the normlist of its first layers.\n",
        "If it matches, the we are safe to conclude that the NN are indeed the same"
      ],
      "metadata": {
        "id": "5MKrd6rcV46i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the norm of the 1st layer of the initial lightweight model\n",
        "print(\"Norm of 1st layer of student: \", torch.norm(student.features[0].weight).item())\n",
        "\n",
        "# print the norm of the 2nd layer of the initial lightweight model\n",
        "print(\"Norm of 2nd layer of student: \", torch.norm(new_student.features[0].weight).item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH73sFxBWKrm",
        "outputId": "d3bf262a-8d8a-4ffa-b090-2c65096d8d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm of 1st layer of student:  2.3673043251037598\n",
            "Norm of 2nd layer of student:  2.2718844413757324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the total number of parameters in each model\n",
        "total_params_teacher = \"{:,}\".format(sum(p.numel() for p in teacher.parameters()))\n",
        "print(f\"Teacher parameters: {total_params_teacher}\")\n",
        "total_params_student = \"{:,}\".format(sum(p.numel() for p in student.parameters()))\n",
        "print(f\"Sutudent parameters: {total_params_student}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKHInLtuWqDG",
        "outputId": "0c35da75-6bc7-4c75-f11c-eb08c9a1a217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher parameters: 1,186,986\n",
            "Sutudent parameters: 267,738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now train the student model\n",
        "train(model=student,\n",
        "      train_loader=train_loader,\n",
        "      epochs=11,\n",
        "      learning_rate=0.001,\n",
        "      device=device)\n",
        "test_accuracy_student = test(model=student,\n",
        "                             test_loader=test_loader,\n",
        "                             device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h26nKe9Z98s",
        "outputId": "12c0063e-8f5a-42c4-98d9-d36dee25598f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11, Loss: 1.4168188462934226\n",
            "Epoch 2/11, Loss: 1.0798806783640782\n",
            "Epoch 3/11, Loss: 0.9477588265295833\n",
            "Epoch 4/11, Loss: 0.8508702839731865\n",
            "Epoch 5/11, Loss: 0.7718450138940836\n",
            "Epoch 6/11, Loss: 0.6993597454350927\n",
            "Epoch 7/11, Loss: 0.633639339557694\n",
            "Epoch 8/11, Loss: 0.5766048659487149\n",
            "Epoch 9/11, Loss: 0.5212877927076481\n",
            "Epoch 10/11, Loss: 0.4770622678729884\n",
            "Epoch 11/11, Loss: 0.4244784154093174\n",
            "Test acc: 90.144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test Accuracy Teacher: {test_accuracy_teacher:.3f}%\")\n",
        "print(f\"Test Accuracy Student: {test_accuracy_student:.3f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdyhP5PGbQx_",
        "outputId": "1ecc214d-6d91-455b-fa32-cdc7cfdbdb32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy Teacher: 97.508%\n",
            "Test Accuracy Student: 90.144%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge distilation run\n",
        "\n",
        "Now let's try to improve the test accuracy of the student network by incorporating the teacher.\n",
        "Knowledge distilation is a straightforward techinique to achieve this, based on the fact that both networks ouput a probability distribuition over our classes.\n",
        "Therefore, the two networks share the same number fo output neurons (THIS IS AN IMPORTANT POINT - CHEK THIS TO OD).\n",
        "The methods works by incorporating an additional loss into the traditional cross entropy loss, which is based on the softmax output of the teacher network.\n",
        "The assumption is that the output activations of a properly trained teacher network carry additional information that can be leveraged by a student network during training.\n",
        "The original work suggest that utilizing ratios of smaller probabilities in the soft targets cal help achieve the underlying objective of DNN, which is to build a similarity structure over the data where similar objects are mapped closer together.\n",
        "For example, in CIFAR-10, a truck could be mistaken for an automobile or airplane, if its wheels are present, but is less mistaken for a dog. Therefore, it makes sense to assume that valuable information resides not only in the top prediction of a properly trained model but in the entire output distribuition.\n",
        "However, cross entropy anole does not significantly exploit this information as the activations for non-predicted classes tend be so small that propagated gradients do not meaningfully change the weights to construct this desirable vector space.\n",
        "\n",
        "As we continue defining our first helper function that introduces a teacher-stuent dynamic, we need to include a few extra parameters.\n",
        "\n",
        "* **T**: Temperature controls the smoothness of the output distributions. Larger **T** leads to smoother distributions, thus smaller probabilities get a larger boost.\n",
        "* **soft_target_loss_weight**: A weight assigned to the extra objective we are about to include\n",
        "* **ce_loss_weight**: A weight assigned to croos-entropy. Tunning these weights pushes the network towards optimizing for either objecitve.\n",
        "\n",
        "1. **The distilation loss is calculated from the logits of the NN. It only returns gradients to the student.**\n",
        "2. Fromt the output layer - get the gradient of all classes - and distilate the loss\n"
      ],
      "metadata": {
        "id": "QWgsRGFWbeLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definng a training loop for distilation from the last layer\n",
        "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
        "  ce_loss = nn.CrossEntropyLoss()  # for classification --> this can be a setted parameter in further versions\n",
        "  optimizer = optim.Adam(student.parameters(), lr=learning_rate)  # seted hyperparameters\n",
        "\n",
        "  teacher.eval()  # to perform this step the teacher should already be trained\n",
        "  student.train()  # student was trained once - to compare results\n",
        "\n",
        "  for epoch in range(epochs): # epochs --> hypeparameters --> in further versions --> set the early stoping\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:  # get the train examples\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()  # zeroing the gradients\n",
        "\n",
        "      # forward pass with teacher model\n",
        "      with torch.no_grad():\n",
        "        teacher_logits = teacher(inputs)  # get the predictions --> without acvtivatons --> raw values\n",
        "\n",
        "      # Now get for the same input, the student logits\n",
        "      student_logits = student(inputs)\n",
        "\n",
        "      # Now, soften the logits values --> in these example applying softmax activation function --> can be tested using several\n",
        "      soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)  # here the teacher - must be the best\n",
        "      soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)  # for the student\n",
        "\n",
        "      # print(f\"Type soft_targets: {type(soft_targets), soft_targets} \\n\")\n",
        "      # print(f\"Type soft_probs: {type(soft_prob), soft_prob} \\n\")\n",
        "\n",
        "      # print(f\"Soft targets shape: {soft_targets.shape}\")\n",
        "      # print(f\"Soft probs shape: {soft_prob.shape}\")\n",
        "\n",
        "      # now calculate the soft targets loss. Scaled by T**2 as sugested by authors of the paper\n",
        "      soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)  # study this formula and possible others\n",
        "\n",
        "      # now calculate the true label loss\n",
        "      label_loss = ce_loss(student_logits, labels)  # cross entropy loss\n",
        "\n",
        "      # print(f\"Type soft_targets_loss: {type(soft_targets_loss), soft_targets_loss} \\n\")\n",
        "      # print(f\"Type label_loss: {type(label_loss), label_loss} \\n\")\n",
        "\n",
        "      # print(f\"soft_targets_loss shape: {soft_targets_loss.shape}\")\n",
        "      # print(f\"label_loss shape: {label_loss.shape}\")\n",
        "      # input()\n",
        "\n",
        "      # weighted sum of the two losses\n",
        "      loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss  # the entire loss is this expression\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "id": "pYvkyCFGfEaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuer from the end of the section\n",
        "* Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss."
      ],
      "metadata": {
        "id": "LeHJ9jBLiE02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_knowledge_distillation(teacher=teacher,\n",
        "                             student=new_student,\n",
        "                             train_loader=train_loader,\n",
        "                             epochs=11,\n",
        "                             learning_rate=0.001,\n",
        "                             T=2,\n",
        "                             soft_target_loss_weight=0.25,\n",
        "                             ce_loss_weight=0.75,\n",
        "                             device=device\n",
        "                             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRCE_eE43S5n",
        "outputId": "381b2054-3764-44b9-f2ae-5ae0fc8c4ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11, Loss: 1.7091755077357182\n",
            "Epoch 2/11, Loss: 1.4989796847181247\n",
            "Epoch 3/11, Loss: 1.3373480530651025\n",
            "Epoch 4/11, Loss: 1.217679157891237\n",
            "Epoch 5/11, Loss: 1.1024534967549318\n",
            "Epoch 6/11, Loss: 1.0127524821197285\n",
            "Epoch 7/11, Loss: 0.9249742182395647\n",
            "Epoch 8/11, Loss: 0.85430480467389\n",
            "Epoch 9/11, Loss: 0.7873108614512416\n",
            "Epoch 10/11, Loss: 0.7221614835817186\n",
            "Epoch 11/11, Loss: 0.6685162819636142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_light_ce_and_kd = test(new_student, test_loader, device)"
      ],
      "metadata": {
        "id": "RDTHEz0-4KGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now compare the student test accuracy with and without the teacher, after disitilation\n",
        "print(f\"Teacher accuracy: {test_accuracy_teacher:.3f} %\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_student:.3f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.3f}\")"
      ],
      "metadata": {
        "id": "aZkHJNHg4ouE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine Loss Minimization Run\n",
        "\n",
        "Feel free to play around with the temperature that controls the softness of the softmax function and the loss coeficients.\n",
        "In NN, it is easy to include additional loss functions to the main objectives to achieve goals like better generalization.\n",
        "Let's try to including an objectve for the student, but now focus on their hidden states rather than their output layers.\n",
        "The goal is to convey information from the teacher's representation to the student by including a **naive loss function**, whose minimization implies that the flattened vectors that are subsequently passed to the classifiers have become more *similar* as the loss decrease.\n",
        "Of course, the teacher does not update its weights, so the minimization depends only on the student weights.\n",
        "The rationale behind this method is that we are operating under the assumption that the teacher model has a better internal representation that is unlikely to be achieved by the student without external intervention, therefore we artificially push the student to mimic the internal representation of the teacher.\n",
        "Whether or not this will end up helping the students is not straightfoward, though, because pushing the lightweight network to reach this point could be a good thing, assuming that we have found an internal representation that leads to better test accuracy, but it could also be harmful because the networks have different architectures and the student does not have the same learning capacity as the teacher.\n",
        "In other words, there is no reason for these two vectors, the student's and the teacher's to match per component.\n",
        "The student could rach an internal representation that is a permutation of the teacher's and it would be just efficient. Nonetheless, we can still run a quick experiment to figure out the inpact of this method.\n",
        "We will be using the **CousineEmbeddingLoss** which is given by the following formula:\n",
        "\n",
        "\n",
        "    loss(x,y) = { 1- cos(x1,x2),    if y=1, or\n",
        "                {max(0,cos(x1,x2) - margin), if y= -1\n",
        "\n",
        "\n",
        "Obviously, there is one thing that we need to resolve first.\n",
        "When we applied distillation to the output layer we mentioned that both networks have the same number of neurons, equal to the number of classes.\n",
        "Howevder, this is not the case for layer following our Conv layers.\n",
        "**Here, the teacher has more neurons that the student after the flattening of the final conv layer**.\n",
        "*Our loss function accpets two vectors of equal dimensionallity as inputs, therefore we need to somehow match them*.\n",
        "We will solve this by *including* an **average pooling layer** *after the teacher Conv layers* to reduce its dimensionality to match that of the student.\n",
        "\n",
        "To proceed, we will modify our model classes, or build a new ones. Now the **forward function** returns not only the logits of the network but also the **flattened hidden** representation after the Conv layer. We include the aforementioned *pooling* for the modified teacher."
      ],
      "metadata": {
        "id": "v8GW-O8K6KiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# buidl a modified network addding average pooling layer after teacher conv layer --> to format the output tensor of the teacher in the same format of the student\n",
        "# to build two vectors with the same dimensionality --> Loss function only accepts vectors with same dimensionality\n",
        "\n",
        "class ModifiedDeepCossine(nn.Module):\n",
        "  def __init__(self, num_classes=10):  # the num classes can be setted as a hyperparameter --> se Daniels code to extract the number of classes\n",
        "    super(ModifiedDeepCossine, self).__init__()  # initializating the class --> get all parents dependencies\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),  # 2D max pooling operation --> over input signal --> Several input planes -->\n",
        "                                                # Selects the maximum values with each pooling region (kernel) --> Passes it to the next layer -> Reduces the size of the tensor\n",
        "        nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,32, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2) # stride=2, pass two by two pixels each step\n",
        "\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(2048,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.features(x)\n",
        "    flattened_conv_output = torch.flatten(x,1)  # get 2D and turn it in 1D\n",
        "    x = self.classifier(flattened_conv_output)\n",
        "    flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output,2)\n",
        "\n",
        "    return x, flattened_conv_output_after_pooling  # x is the predicted and flattened_conv_output_after_pooling is the vector with dimensionality adjusted to be used in the loss function\n",
        "                                                   # This value is taken after the conv steps not after the classification step --> here is extracted the features, for adjust prediction"
      ],
      "metadata": {
        "id": "eTlgOMWQWyoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a modified student class where return a tuple. We do not apply pooling after flattening\n",
        "class ModifiedLightDeepCossine(nn.Module):\n",
        "  def __init__(self,num_classes=10):\n",
        "    super(ModifiedLightDeepCossine, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(16,16, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(1024, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(256, num_classes)  # num_classes --> Hyperparameter\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    flattened_conv_output = torch.flatten(x,1)  # flatten the tensor\n",
        "    x = self.classifier(flattened_conv_output)\n",
        "\n",
        "    return x, flattened_conv_output  # flattened_conv_output will be used to adjust loss function"
      ],
      "metadata": {
        "id": "zGsfeBBOfAUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We do not have to train the modified DNN from scratch of course, it is needed just load its weights\n",
        "modified_teacher = ModifiedDeepCossine(num_classes=10).to(device)\n",
        "modified_teacher.load_state_dict(teacher.state_dict())"
      ],
      "metadata": {
        "id": "ce5IsrSgdbvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once again to ensure the norm of the 1st layer is the same for both networks\n",
        "print(\"Norm of 1st layer for deep_nn:\", torch.norm(teacher.features[0].weight).item())\n",
        "print(\"Norm of 1st layer for deep_nn\", torch.norm(modified_teacher.features[0].weight).item())"
      ],
      "metadata": {
        "id": "Ir0LPZJPd415"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a modified lightweight network with the same seed as the other lightweight instances.\n",
        "# This will be trained from scratch to examine the effectiveness of cousine loss minimization\n",
        "modified_student = ModifiedLightDeepCossine(num_classes=10).to(device)\n",
        "print(\"Norm of 1st layer of modified student: \", torch.norm(modified_student.features[0].weight).item())"
      ],
      "metadata": {
        "id": "FYdHfCHCemmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it is needed to modify the train loop because the model returns a tuple (logits, hidden_representation).\n",
        "\n",
        "Using a sample input tensor we can print their shapes"
      ],
      "metadata": {
        "id": "9jwyXDwuiJR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a sample input tensor\n",
        "sample_input = torch.rand(128,3,32,32).to(device)  # batch size: 128, filters: 3, image size: 3x3"
      ],
      "metadata": {
        "id": "KJkWPuM4n9zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the input through the student\n",
        "logits, hidden_representation = modified_student(sample_input)"
      ],
      "metadata": {
        "id": "FzR4C1m_oMAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of the tensors\n",
        "print(\"Stutent logits shape: \", logits.shape)  # batch_size x total_classes\n",
        "print(\"Stutent hidden representation shape: \", hidden_representation.shape)"
      ],
      "metadata": {
        "id": "9XMlWCxSoa0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the input through the teacher\n",
        "logits, hidden_representation = modified_teacher(sample_input)"
      ],
      "metadata": {
        "id": "2CI0WJzkp7Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the teacher shapes\n",
        "print(\"Teacher Logits shape: \", logits.shape)\n",
        "print(\"Teacher hidden representation shape: \", hidden_representation.shape)"
      ],
      "metadata": {
        "id": "nmS_cPJGqIG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, **hidden_representation_size** is **1024**. This is flattened feature map of the final Conv layer of the student and as you can see, it is the input for its classifier.\n",
        "It is **1024** for the teacher too, because we made it so with **avg_pool1d** from **2048**. The loss applied here only affects the weights of the student priot to the loss calculation.\n",
        "In other words, it does not affect the classifier of the student.\n",
        "The modified training loop is the following.\n",
        "\n",
        "In Cosine Loss minimization, we want to maximize the cosine similarity of the two representation by returning gradients to the student:"
      ],
      "metadata": {
        "id": "lf8OA1fjqVMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n",
        "  ce_loss = nn.CrossEntropyLoss()  # for classification\n",
        "  cosine_loss = nn.CosineEmbeddingLoss()  # for features after conv layers\n",
        "  optimizer = optim.Adam(student.parameters(), lr= learning_rate)  # can include learning_rate decay\n",
        "\n",
        "  teacher.to(device)\n",
        "  student.to(device)\n",
        "  teacher.eval()  # after had trained the teacher\n",
        "  student.train()\n",
        "\n",
        "  # now the loop to iterate the data\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      # zeroing the optmizer graph before to train\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #forward pass with the teacher model and keep only the hidden representation\n",
        "      with torch.no_grad():\n",
        "        _, teacher_hidden_representation = student(inputs)  # get the features --> hidden repressentation\n",
        "\n",
        "      # Forward pass with the student model\n",
        "      student_logits, student_hidden_representation = student(inputs)  # get the ouptut values after classification, and the hidden representation to distile knowledge\n",
        "\n",
        "      # Calculate the cosien loss. Target is a vector of ones.\n",
        "      # from the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n",
        "      hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n",
        "\n",
        "      # Calculate the true label loss\n",
        "      label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "      # weighted sum of the two losses\n",
        "      loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "id": "M27H6gf9rVsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is needed to modify the test function for the same reason.\n",
        "Here we ignore the hidden representation returned by the model."
      ],
      "metadata": {
        "id": "qJDC1WsD5j9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to test the models with Cosine Loss Function\n",
        "def test_multiple_outputs(model, test_loader, device):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0  # sum corrects\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():  # put the model in evaluation mode\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs, _ = model(inputs)  # disregard the second tensor of the tuple - get the logits\n",
        "      _ , predicted = torch.max(outputs.data,1) # get the maximum values --> predict the class\n",
        "\n",
        "      total += labels.size(0) # get the total of predictios\n",
        "      correct += (predicted == labels).sum().item()   # calculate the total of corrects\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f\"Test Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "eAZtNG_YSsK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we could easily include both KD and cossine loss minimization in the same function.\n",
        "It is common to combine methods to achieve better performance in teacher-student paradigms.\n",
        "For now, we can run a simple train-test session."
      ],
      "metadata": {
        "id": "cjXrR1Tla2kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test the lightweight network with cross entropy loss\n",
        "train_cosine_loss(teacher=modified_teacher,\n",
        "                  student=modified_student,\n",
        "                  train_loader=train_loader,\n",
        "                  epochs=11,\n",
        "                  learning_rate=0.001,\n",
        "                  hidden_rep_loss_weight=0.25,\n",
        "                  ce_loss_weight=0.75,\n",
        "                  device=device)"
      ],
      "metadata": {
        "id": "A05MWlc0boj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_teacher, test_loader, device)"
      ],
      "metadata": {
        "id": "XoJD9S69cRr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intermediate regressor run\n",
        "\n",
        "Our naive minimization does not guarantee better results for several reasons, one being the dimensionality of the vectors.\n",
        "Cosine similarity generally works better than Euclidean distance for vectors of higher dimensionality, but we were dealing with vectors with 1024 components each, so it is much harder to extract meaningful similirarities.\n",
        "Furthemore, as we mentioned, pushing towards a match of the hidden representation of the teacher and the student is not supported by theory.\n",
        "There are no good reasons why we should be amiming for a 1:1 match of these vectors. We will provide a final example of training intervention by including an extra network called regressor.\n",
        "The objective is to first extract the feature map of the teacher after conv layer, then extract a feature map of the student after conv layer, and finally try to match these maps.\n",
        "However, this time, we will introduce a regressor between the networks to facilitate the matching process.\n",
        "The regressor will be trainable and ideally do a better job than out naive cosine loss minimization scheme.\n",
        "Its main job is to match the dimensionality of these feature maps so that we can properly define a loss function between the teacher and the student.\n",
        "Defining such a loss function provides a teaching \"path\", which is basically a flow to back-propagate gradients that will change the student's weights. Focusing on the output of the Conv layers right before each classifier for our original networks, we have the following shapes:\n",
        "\n",
        "1. Naive minimization does not guarantee better results.\n",
        "2. Cosine similarity generall works better that Euclidian distance, for vectors of higher dimensionality - 1024 components --> harder to extract meaningfull similirarities\n",
        "3. match a hidden representation --> teacher --> student are not supported by theory\n",
        "4. Objective --> Extract feature maps --> Introduce a regressor between networks to facilitate the matching process.\n",
        "5. The regressor will be trainable --> Ideally do a better job than our naive cosine loss minimization\n",
        "6.  Main job --> match the dimensionality --> can properly define a loss function --> Between teacher and student\n",
        "7. Defining a loss function --> provides a \"path\" --> which is basically back-propagate gradients that will change the student weights.\n",
        "8. Focusing on ouput of Conv layers tight before classifier for original networtks\n",
        "\n"
      ],
      "metadata": {
        "id": "7TTcB4u3ekWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the sample input only from the Conv feature extractor\n",
        "convolutional_fe_output_student = student.features(sample_input)\n",
        "convolutional_fe_output_teacher = teacher.features(sample_input)"
      ],
      "metadata": {
        "id": "Fb-5cI47pPxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Student feature extractor output shape: {convolutional_fe_output_student.shape}\")\n",
        "print(f\"Teacher feature extractor output shape: {convolutional_fe_output_teacher.shape}\")"
      ],
      "metadata": {
        "id": "FlDq8Xrntmc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 32 filters for the teacher and 16 filters for the student.\n",
        "We will include a trainable layer that converts the feature map of the student to the shape of the feature of the teacher.\n",
        "In practice, we modify the lightweight class to return the hidden state after an intermediate regressor that matches the sizes of the Conv feature maps and the teacher class to return the output of the final Conv layer without pooling or flattening.\n",
        "\n",
        "calculate the loss function in the MaxPool2d function\n",
        "\n",
        "The trainable layer matches the shapes of intermediate tensors and MSE is properly defined:\n"
      ],
      "metadata": {
        "id": "YO6F9DF7txIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedDeepNNRegressor(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(ModifiedDeepNNRegressor, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128,64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,32, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "    self.classifier = nn.Sequential(  # in other examples can be used another backbone\n",
        "        nn.Linear(2048, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    conv_features_maps = x  # get the output of the convolutional step\n",
        "    x = torch.flatten(x,1)  # flaten the feature maps before to input it in the classsifier\n",
        "    x = self.classifier(x)  # made the prediction\n",
        "\n",
        "    return x, conv_features_maps"
      ],
      "metadata": {
        "id": "HGRnLL5hvmg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedLightNNRegressor(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(ModifiedLightNNRegressor, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(16,16, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    # Include an extra regressor - in this case linear\n",
        "    self.regressor = nn.Sequential(\n",
        "        nn.Conv2d(16,32, kernel_size=3, padding=1)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(1024, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),  # Also a Hyperparameters\n",
        "        nn.Linear(256, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    regressor_output = self.regressor(x)  # Trainable layer convert the feature maps of the student to the shape of the teacher\n",
        "                                          # Modify the lightweight class to return the hidden state after intermediate regressor that matches the size of Conv feature maps\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x, regressor_output"
      ],
      "metadata": {
        "id": "03dwIULgxkb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, we have to update our train loop again. This time, we extract the regressor output of the stundent, the feature map of the teacher, we calculate the **MSE** on these tensors - they have the exact same shape so it is properly defined - and back propagate gradients based on that loss, in addition to the regular cross entropy loss of the classification task."
      ],
      "metadata": {
        "id": "WSJPQx8yz34V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mse_loss(teacher, student, train_loader, epochs, learning_rate, feature_map_weight, ce_loss_weight, device):\n",
        "  ce_loss = nn.CrossEntropyLoss()  # Hyperparameters\n",
        "  mse_loss = nn.MSELoss()  # Hyperparameters\n",
        "  optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "  teacher.to(device)\n",
        "  student.to(device)\n",
        "  teacher.eval()  # evaluation mode\n",
        "  student.train()  # student to train mode\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # again ignore the teacher logits\n",
        "      with torch.no_grad():\n",
        "        _, teacher_feature_map = teacher(inputs)  # get the feature maps from the teacher - not the logits\n",
        "\n",
        "      # Forward pass with the student model.\n",
        "      student_logits, regressor_feature_map = student(inputs)  # get the logits and feature maps from student\n",
        "\n",
        "      # Calculate the loss\n",
        "      hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n",
        "\n",
        "      # Calulate the ture label loss\n",
        "      label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "      # weighted sum of the two losses\n",
        "      loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss # study why this configuration of the sum and multiplication of the losses\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "Wr97Sww44Mu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ModifiedLightNNRegressor\n",
        "modified_student_reg = ModifiedLightNNRegressor(num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "YRGddVIq9z5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not have to train the modified DNN from scratch, load the state_dict --> weight and biases\n",
        "modified_teacher_reg = ModifiedDeepNNRegressor(num_classes=10).to(device)\n",
        "modified_teacher_reg.load_state_dict(teacher.state_dict())"
      ],
      "metadata": {
        "id": "DFoVGY4s-Vbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test once again\n",
        "train_mse_loss(teacher=modified_teacher_reg,\n",
        "               student=modified_student_reg,\n",
        "               train_loader=train_loader,\n",
        "               epochs=10,\n",
        "               learning_rate=0.001,\n",
        "               feature_map_weight=0.25,\n",
        "               ce_loss_weight=0.75,\n",
        "               device=device)"
      ],
      "metadata": {
        "id": "0IWtQPQ0_Lml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_light_ce_and_mse_loss = test_multiple_outputs(modified_student_reg, test_loader, device)"
      ],
      "metadata": {
        "id": "je926l7s_sCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is expected that the final method will work better than **CosineLoss** Because now we have allowed a trainable layer between the teacher and the student.\n",
        "Which gives the student some wiggle room when it comes to learning, rather than pushing the student to copy the teacher's representation.\n",
        "Including the extra network is the idea behind hint-based distilation."
      ],
      "metadata": {
        "id": "1Le6qSUrASlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Teacher accuracy: {test_accuracy_teacher:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_student:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
        "print(f\"Student accuracy with CE + CosineLoss: {test_accuracy_light_ce_and_cosine_loss:.2f}%\")\n",
        "print(f\"Student accuracy with CE + RegressorMSE: {test_accuracy_light_ce_and_mse_loss:.2f}%\")"
      ],
      "metadata": {
        "id": "DIg9eOMxAuJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "None of these methods above increases the number of parameters of the NN or inference time.\n",
        "So the performance increase comes at a little cost of calculating gradients during training.\n",
        "In ML app, we most care about inference time because training happens before the model deployment.\n",
        "If your light weight model is still to heavy for deployment, we can apply different ideas\n",
        "--> Such post-training quantization.\n",
        "Additional losses can be applied in many tasks, no just classification --> can experiment with quantities like coeficients, temperatures, or number of neurons\n",
        "Tune any number of the tutorial"
      ],
      "metadata": {
        "id": "MhHt-9OhBTUQ"
      }
    }
  ]
}