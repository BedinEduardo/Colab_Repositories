{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfVnyilqLn6veAhja/bClW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BedinEduardo/Colab_Repositories/blob/master/PyTorch_Backends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backend OverView\n",
        "\n",
        "ExecuTorch backends provide hardware acceleration for a specific hardware target.\n",
        "In order to achieve maximum performance on target hardware, ExecuTorch optimizes the model for a specifc backend during the export and lowering process.\n",
        "This means that the resulting .pte file is specialized for the specific hardware.\n",
        "In order to deploy to multiple backends, such as Core ML on iOS and Arm CPU or Android. - Generate a dedicated .pte file for each.\n",
        "\n",
        "## XNNPACK Backend\n",
        "\n",
        "The XNNPACK Backend delegate is the ExecuTorch solution for CPU execution on mobile CPUs.\n",
        "\n",
        "### Features\n",
        "* Wide operator supports on Arm and x86 CPU, available on any modelr mobile phone\n",
        "* Support for a wide variety of quantization schemes and quantized operators.\n",
        "* Supports fp32 and fp16 activations\n",
        "* Supports 9-bit quantization\n",
        "\n",
        "### Target Requirements\n",
        "* ARM64 on Android, iOS, macOS, Linux, Windows\n",
        "* ARMv6 on Linux\n",
        "* x86 and x86-64 Windows, linux, macOS, Androind, iOS simulator\n",
        "\n",
        "### Usinf the XNNPACK Backend\n",
        "\n",
        "To target XNNPACK backend during the export and lowering process, pass an instance of the `XnnpackPartioner` to `to_edge_transform_and_lower`"
      ],
      "metadata": {
        "id": "aJK-IfvZop4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install executorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yHRAhp-ysu6",
        "outputId": "29596aef-fc0a-4c07-b1ae-c60f0a5cdf6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: executorch in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: expecttest in /usr/local/lib/python3.11/dist-packages (from executorch) (0.3.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from executorch) (25.2.10)\n",
            "Requirement already satisfied: hypothesis in /usr/local/lib/python3.11/dist-packages (from executorch) (6.135.32)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from executorch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from executorch) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from executorch) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from executorch) (2.2.2)\n",
            "Requirement already satisfied: parameterized in /usr/local/lib/python3.11/dist-packages (from executorch) (0.9.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from executorch) (8.3.5)\n",
            "Requirement already satisfied: pytest-xdist in /usr/local/lib/python3.11/dist-packages (from executorch) (3.8.0)\n",
            "Requirement already satisfied: pytest-rerunfailures in /usr/local/lib/python3.11/dist-packages (from executorch) (15.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from executorch) (6.0.2)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.11/dist-packages (from executorch) (0.18.14)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from executorch) (1.14.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from executorch) (0.9.0)\n",
            "Requirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.11/dist-packages (from executorch) (2.7.0)\n",
            "Requirement already satisfied: torchaudio==2.7.0 in /usr/local/lib/python3.11/dist-packages (from executorch) (2.7.0)\n",
            "Requirement already satisfied: torchvision==0.22.0 in /usr/local/lib/python3.11/dist-packages (from executorch) (0.22.0)\n",
            "Requirement already satisfied: torchao==0.10.0 in /usr/local/lib/python3.11/dist-packages (from executorch) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from executorch) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->executorch) (3.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.22.0->executorch) (11.2.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->executorch) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->executorch) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->executorch) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->executorch) (2025.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis->executorch) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis->executorch) (2.4.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->executorch) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->executorch) (1.6.0)\n",
            "Requirement already satisfied: execnet>=2.1 in /usr/local/lib/python3.11/dist-packages (from pytest-xdist->executorch) (2.1.1)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml->executorch) (0.2.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->executorch) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0->executorch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
        "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
        "from executorch.exir import to_edge_transform_and_lower"
      ],
      "metadata": {
        "id": "7AyFWfJIyMQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_v2 = models.mobilenetv2.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).eval()\n",
        "sample_inputs = (torch.randn(1,3,224,224),)"
      ],
      "metadata": {
        "id": "SSNPufSoywha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "et_program = to_edge_transform_and_lower(\n",
        "    torch.export.export(mobilenet_v2, sample_inputs),\n",
        "    partitioner=[XnnpackPartitioner()],\n",
        ").to_executorch()"
      ],
      "metadata": {
        "id": "7Qwld5kDzGdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"mv2_xnn_pack.pte\",\"wb\") as file:\n",
        "  et_program.write_to_file(file)"
      ],
      "metadata": {
        "id": "zD8DdMSezibP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partioner API\n",
        "\n",
        "The XNNPACK partioner API allows for configuration of the model delegation to XNNPACK.\n",
        "Passing the `XnnpackPartioner` instance with no additonal parameters will run as much of the model as possible on the XNNPACK backend.\n",
        "\n",
        "* `configs`: Control which operators are delegated to XNNPACK\n",
        "* `configs_precision`: filter operators by data type. Default `ConfigPrecisionType.FP32`, `ConfigPrecisionType.STATIC_QUANT`, or `ConfigPrecisionType.DYNAMIC_QUANT`\n",
        "* `per_op_mode`: If true, emit individual delegate calls for every operator.\n",
        "* `verbose`: If true, print additional information during lowering.\n",
        "\n",
        "## Testing the Model\n",
        "\n",
        "After generating the XNNPACK-delegate .pte, the model can be tested from Python using the executorch runtime python bindings.\n",
        "\n",
        "## Quantization\n",
        "\n",
        "The XNNPACK delegate can also be used as a backend to execute symmetrically quantized models.\n",
        "`XNNPACKQuantizer`. `Quantizers` are backend specific, which means the `XNNPACKQuantizer`is configured to quantize to leverage the quantized operators offered by the XNNPACK library.\n",
        "\n",
        "### Supported Quantization Schemes\n",
        "\n",
        "The XNNPACK delegate supports the following quantization schemes:\n",
        "* 8-bit symmetric weights with 8-bit assymetric activations - PF2E quantization flow.\n",
        "\n",
        "### 8-bit Quantization using PT2 Flow\n",
        "\n",
        "1. Build instance `XnnpackQuantizer` class\n",
        "2. `torch.export.export_for_trainig` prepare for quantization\n",
        "3. `prepare_pt2e` model for quantization\n",
        "4. For static quantization, run the prepared model with representative samples to calibrate the quantized tensor activation ranges.\n",
        "5. `convert_pt2e`\n",
        "6. Export and lower the model using the standard flow.\n"
      ],
      "metadata": {
        "id": "vF-QVnTpzwSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
        "from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import XNNPACKQuantizer\n",
        "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
        "from executorch.exir import to_edge_transform_and_lower\n",
        "from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e\n",
        "from torch.ao.quantization.quantizer.xnnpack_quantizer import get_symmetric_quantization_config"
      ],
      "metadata": {
        "id": "7Ly4gUYw_tJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenetv2.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).eval()\n",
        "sample_inputs = (torch.randn(1, 3, 224, 224), )"
      ],
      "metadata": {
        "id": "14jvVvUCBR24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qparams = get_symmetric_quantization_config(is_per_channel=True)\n",
        "quantizer = XNNPACKQuantizer()\n",
        "quantizer.set_global(qparams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0aO__0zBRn-",
        "outputId": "f12e11f4-74ce-4838-b628-7af4713b8205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<executorch.backends.xnnpack.quantizer.xnnpack_quantizer.XNNPACKQuantizer at 0x7ba4c6884a10>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ep = torch.export.export_for_training(model, sample_inputs).module()\n",
        "prepared_model = prepare_pt2e(train_ep, quantizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLGLU7s2Gmk3",
        "outputId": "343c10cf-4a9e-44e1-993f-99bb7234076b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_1) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_2) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_3) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_4) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_5) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_6) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_7) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_8) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_9) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_10) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_11) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_12) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_13) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_14) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_15) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_16) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_17) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_18) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_19) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_20) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_21) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_22) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_23) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_24) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_25) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_26) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_27) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_28) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_29) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_30) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_31) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_32) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_33) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_34) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_35) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_36) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_37) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_38) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_39) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_40) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_41) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_42) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_43) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_44) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_45) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_46) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_47) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_48) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_49) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_50) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/fx/graph.py:1179: UserWarning: erase_node(batch_norm_51) on an already erased node\n",
            "  warnings.warn(f\"erase_node({to_erase}) on an already erased node\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for cal_sample in [torch.randn(1,3,224,224)]:\n",
        "  prepared_model(cal_sample)"
      ],
      "metadata": {
        "id": "PUzGw5ic47wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = convert_pt2e(prepared_model)\n",
        "\n",
        "et_program = to_edge_transform_and_lower( # (6)\n",
        "    torch.export.export(quantized_model, sample_inputs),\n",
        "    partitioner=[XnnpackPartitioner()],\n",
        ").to_executorch()"
      ],
      "metadata": {
        "id": "rW0WAgZpHIRT",
        "outputId": "34034765-74d5-4a85-ecab-a123aedce8b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GraphModule' object has no attribute '_frozen_param0'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-25-1351010314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantized_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_pt2e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m et_program = to_edge_transform_and_lower( # (6)\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXnnpackPartitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/quantize_pt2e.py\u001b[0m in \u001b[0;36mconvert_pt2e\u001b[0;34m(model, use_reference_representation, fold_quantize)\u001b[0m\n\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[0moriginal_graph_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_reference_decomposed_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fold_conv_bn_qat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/quantize_fx.py\u001b[0m in \u001b[0;36m_convert_to_reference_decomposed_fx\u001b[0;34m(graph_module, convert_custom_config, qconfig_mapping, backend_config)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;34m\"quantization_api.quantize_fx._convert_to_reference_decomposed_fx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     )\n\u001b[0;32m--> 720\u001b[0;31m     return _convert_fx(\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0mgraph_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0mis_reference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/quantize_fx.py\u001b[0m in \u001b[0;36m_convert_fx\u001b[0;34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[0m\n\u001b[1;32m    539\u001b[0m     }\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     quantized = convert(\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0mgraph_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mis_reference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/fx/convert.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(model, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;31m# remove deadcode after converting observers to quant/dequant ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meliminate_dead_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;31m# TODO: maybe move this to quantize_fx.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, graph, class_name)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"get_attr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"call_module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                     \u001b[0m_copy_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mtargets_to_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m_copy_attr\u001b[0;34m(from_module, to_module, target)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mfrom_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;31m# If it is a tensor and not a parameter attribute of a module, it should be a named buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# So, we register it as a named buffer in the target module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1941\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GraphModule' object has no attribute '_frozen_param0'"
          ]
        }
      ]
    }
  ]
}