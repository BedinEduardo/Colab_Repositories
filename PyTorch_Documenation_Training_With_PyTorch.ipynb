{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6Uyyv6gTLGS74OQAEaTGr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BedinEduardo/Colab_Repositories/blob/master/PyTorch_Documenation_Training_With_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In past videos, we discussed and demostred:\n",
        "\n",
        "* Buildong models with NN layers and functions of the torch.nn module\n",
        "* The mechanics of autograd gradient computation, which is central to gradient-based model traning\n",
        "* Using TensorBoard to visualize training progress and other activities\n",
        "\n",
        "In this video, we will be adding some new tools to your inventory:\n",
        "\n",
        "* We will get familiar with the dataset and dataloader abstractions, and how they ease the process of feeding data to your model during a training loop\n",
        "* We will discuss specific loss functions and when to use them\n",
        "* We will look at PyTorch optimizers, which implement algorithms, which implement algorithms to adjust model weights based on the outcome of a loss function\n",
        "\n",
        "Finally, we will pull all of these togethere and see a full PyTorch training loop in action"
      ],
      "metadata": {
        "id": "kaQiNQQpl301"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and DataLoader\n",
        "\n",
        "The `Dataset` and `Dataloader` classes encapsulate the process of pulling your data from storage and exposing it to your training loop in batches.\n",
        "\n",
        "The `Dataset` is reponsible for accessing and processing single instances of data.\n",
        "\n",
        "The `DataLoader` pulls instance of data from the `Dataset` - either automatically or with a sampler that you define - collects them in batches, and returns them for consuption by your training loop.\n",
        "The `DataLoader` works with all kinds of datasets, regardless of the type of they contain.\n",
        "\n",
        "We use `torchvision.transforms.Normalize()` to zero-center and normalize the distribution of the image tile content, and download both training and validation data splits."
      ],
      "metadata": {
        "id": "r2zQF-qGun_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "w8rmuwPXxhPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch TensorBoard Support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "olRPo1Grxp_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying transforms Compose\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,),(0.5,))]\n",
        ")"
      ],
      "metadata": {
        "id": "RGIqzxQoxw5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build datasets for training and validation, download if necessary\n",
        "training_set = torchvision.datasets.FashionMNIST('./data',\n",
        "                                                  train=True,\n",
        "                                                  transform=transform,\n",
        "                                                  download=True)\n",
        "\n",
        "validation_set = torchvision.datasets.FashionMNIST('./data',\n",
        "                                                   train=False,\n",
        "                                                   transform=transform,\n",
        "                                                   download=True)"
      ],
      "metadata": {
        "id": "cTXnVjIbyJI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c4960c-9774-4f47-e66b-6c9d855630d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.9MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 268kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 4.96MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 11.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the data loaders for our datasets; shuffle for training not for validation\n",
        "training_dataloader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "so573lFFy8Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
      ],
      "metadata": {
        "id": "O3rKjbixy3xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#report the split sizes\n",
        "print(f\"Training set has {format(len(training_set))} instances\")\n",
        "print(f\"Validation set has {format(len(validation_set))} instances\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYzBmZmf0vox",
        "outputId": "f14d5b38-c00e-4bec-f0b7-65b347e1a04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has 60000 instances\n",
            "Validation set has 10000 instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, let's visualize the data as a sanity check:"
      ],
      "metadata": {
        "id": "Q3x0e2BXUX8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Helper function for inline image display\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "  if one_channel:\n",
        "    img = img.mean(dim=0)\n",
        "  img = img / 2 + 0.5  # unnormalize\n",
        "  npimg = img.numpy()\n",
        "  if one_channel:\n",
        "    plt.imshow(npimg, cmap=\"Greys\")\n",
        "  else:\n",
        "    plt.imshow(np.transpose(npimg (1,2,0)))\n",
        "\n",
        "dataiter = iter(training_dataloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Build a grid from the images and show them\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "matplotlib_imshow(img_grid, one_channel=True)\n",
        "print(' '.join(classes[labels[j]] for j in range(4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "FkF317jHUdZ7",
        "outputId": "ed55f83f-63db-407d-83cb-b1b30518f3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shirt Shirt Sneaker Sneaker\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Model\n",
        "\n",
        "The model we will use in this example is a variant of LeNet-5 - it should be familiar if you have watched the previous videos in this series."
      ],
      "metadata": {
        "id": "gEVeRMjoWASO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyTorch models inherit from torch.nn.Module\n",
        "class GarmentClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GarmentClassifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1,6,5)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(6,16,5)\n",
        "    self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 16 * 4 *4)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "dupYr7cQWX-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GarmentClassifier()"
      ],
      "metadata": {
        "id": "-hlKXvSfYpHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzBG-AxvYrRX",
        "outputId": "c1c7e667-9c0f-4810-e431-94ed1717cd8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GarmentClassifier(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function\n",
        "\n",
        "For this example, we will be using a cross-entropy loss.  For demostration purposes, we will build batches of dummy ouput and label values, run them thorugh the loss function, and examine the result."
      ],
      "metadata": {
        "id": "JgTXz5qPaWKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "# Loss function expect data in batches, so we are building batches of 4\n",
        "# Represents the model's confidence in each of the 10 classes, for a given input\n",
        "\n",
        "dummy_outputs = torch.rand(4,10)\n",
        "# Represents the correct class among the 10 being tested.\n",
        "dummy_labels = torch.tensor([1,5,3,7])\n",
        "\n",
        "print(dummy_outputs)\n",
        "print(\"\\n\")\n",
        "print(dummy_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phsd8EYfxH-r",
        "outputId": "a0298379-2bf9-4c41-dd86-32f70e424c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0562, 0.7091, 0.2280, 0.6953, 0.0848, 0.8041, 0.6545, 0.0544, 0.8191,\n",
            "         0.9474],\n",
            "        [0.8819, 0.6581, 0.8237, 0.1040, 0.7286, 0.7849, 0.4379, 0.8986, 0.7640,\n",
            "         0.5325],\n",
            "        [0.0304, 0.8785, 0.2898, 0.0630, 0.1004, 0.2020, 0.9170, 0.3188, 0.6867,\n",
            "         0.4683],\n",
            "        [0.9012, 0.6297, 0.6335, 0.4831, 0.1909, 0.2243, 0.9390, 0.3029, 0.6049,\n",
            "         0.6124]])\n",
            "\n",
            "\n",
            "tensor([1, 5, 3, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(dummy_outputs, dummy_labels)\n",
        "\n",
        "print(f\"Total loss for this batch: {format(loss.item())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H57ArNSHxzDW",
        "outputId": "bdee8de1-d557-4fe0-a2b9-0b940d4070de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss for this batch: 2.4061243534088135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "\n",
        "For this example, we will be using simpe **stochastic gradient descent** with momentum.\n",
        "\n",
        "It can be instructive to try some variations on this optimizations scheme:\n",
        "\n",
        "* Learning rate determines the size of the steps the optimizer takes. What does a different learning rate do to the your training results, in terms of accuracy and convergence time?\n",
        "* Momentum nudges the optimizer in the direction of strongest gradient over multiple steps. What does changing this value do to your results?\n",
        "* Try some different optimization algorithms, such as averaged SGS, Adagrad, or Adam. How do your results differ?"
      ],
      "metadata": {
        "id": "Msyc9NkYxyDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer specified in the torch.optim package\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "EhBftNuKy-mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Loop\n",
        "\n",
        "Below, we have a function that performs one training epoch. It enumerates data from DataLoader, and on each pass of the loop does the following:\n",
        "\n",
        "* Gets a batch of training data from the DataLoader\n",
        "* Zeros the optimizer gradients\n",
        "* Performs an inference - that is, gets predictions from the model for an input batch\n",
        "* Calculates the loss for that set of predictions vs. the labels on the datasset\n",
        "* Calculates the backwards gradients over the learning weights\n",
        "* Tells the optimzer to perform one learning step - that is, adjust the model's learning weights based on the observed gradients for this batch, according to the optimization algorithm we chose.\n",
        "* It reports on the loss for evey 1000 batches.\n",
        "* Finally, it reports the average per-batch loss for the last 1000 batches, for comparision with a validation run"
      ],
      "metadata": {
        "id": "B5VtDBaw045S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "  running_loss = 0.\n",
        "  last_loss = 0.\n",
        "\n",
        "  # Here, we use enumerate - training_loader - instead of iter - training_loader - so that we can track the batch index and do some intra-epoch reporting\n",
        "  for i, data in enumerate(training_dataloader):\n",
        "    # Every data instance is an ouptut + label pair\n",
        "    inputs, labels = data\n",
        "\n",
        "    # Zero your gradients for every batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Make predictions for this batch\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Compute the loss and its gradients\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    # Adjust learning weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Gather data and report\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 # loss per batch\n",
        "      print(f' Batch {format(i+1)} | Loss: {last_loss}')\n",
        "      tb_x = epoch_index * len(training_dataloader) + i +1\n",
        "      tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0.\n",
        "\n",
        "  return last_loss"
      ],
      "metadata": {
        "id": "-5QIOuKl4E1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Per-Epoch Activity\n",
        "\n",
        "There are a couple of things we will want to do once per epoch:\n",
        "\n",
        "* Perform validation by checking our relative loss on a set of a data that was not used for training, and report this\n",
        "* Save a copy of the model.\n",
        "\n",
        "Here, we will do our reporting in TensorBoard. This will require going to the command line to start TensorBoard, and opening it in another browser tab."
      ],
      "metadata": {
        "id": "_se3F0TJ6v3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initailizing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{format(timestamp)}')\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"EPOCH: {format(epoch_number + 1)}\")\n",
        "\n",
        "  # Make sure gradient tracking is on, and do a pass over the data\n",
        "  model.train(True)\n",
        "  avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "  running_vloss = 0.0\n",
        "  # Set the model to evaluation mode, disabling dropout and using population\n",
        "  # statistcs for batch normalization\n",
        "  model.eval()\n",
        "\n",
        "  # Disable gradient computation and reduce memory comsuption\n",
        "  with torch.no_grad():\n",
        "    for i, vdata in enumerate(test_dataloader):\n",
        "      vinputs, vlabels = vdata\n",
        "      voutputs = model(vinputs)\n",
        "      vloss = loss_fn(voutputs, vlabels)\n",
        "      running_vloss += vloss\n",
        "\n",
        "  avg_vloss = running_vloss / (i+1)\n",
        "  print(f\"LOSS train {format(avg_loss)} | valid: {format(avg_vloss)}\")\n",
        "\n",
        "  # Log the running loss averaged per batch for both training and validation\n",
        "  writer.add_scalars('Training vs. Validation Loss',\n",
        "                     { 'Training': avg_loss, 'Validation': avg_vloss},\n",
        "                     epoch_number + 1)\n",
        "  writer.flush()\n",
        "\n",
        "  # Track best performance, and save the model's state\n",
        "  if avg_vloss < best_vloss:\n",
        "    best_vloss = avg_vloss\n",
        "    model_path = f'model_{format(timestamp)}_{format(epoch_number)}'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "  epoch_number +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B36OxRdb8MSA",
        "outputId": "60b25564-1d59-41b0-a953-4f896eb25632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            " Batch 1000 | Loss: 1.622510337010026\n",
            " Batch 2000 | Loss: 0.83273185412772\n",
            " Batch 3000 | Loss: 0.7051361261811108\n",
            " Batch 4000 | Loss: 0.6159963581711054\n",
            " Batch 5000 | Loss: 0.6025558550967834\n",
            " Batch 6000 | Loss: 0.5785097821586532\n",
            " Batch 7000 | Loss: 0.5452367549687624\n",
            " Batch 8000 | Loss: 0.5117050479456083\n",
            " Batch 9000 | Loss: 0.48152009956369873\n",
            " Batch 10000 | Loss: 0.47623462688550355\n",
            " Batch 11000 | Loss: 0.44297481009166223\n",
            " Batch 12000 | Loss: 0.44969412846304474\n",
            " Batch 13000 | Loss: 0.4411469753604615\n",
            " Batch 14000 | Loss: 0.4203222837037756\n",
            " Batch 15000 | Loss: 0.4364258945485344\n",
            "LOSS train 0.4364258945485344 | valid: 0.40859439969062805\n",
            "EPOCH: 2\n",
            " Batch 1000 | Loss: 0.38740461650589714\n",
            " Batch 2000 | Loss: 0.38929962539949337\n",
            " Batch 3000 | Loss: 0.39275878414453474\n",
            " Batch 4000 | Loss: 0.37453301887694396\n",
            " Batch 5000 | Loss: 0.37172354366839866\n",
            " Batch 6000 | Loss: 0.3780099771274545\n",
            " Batch 7000 | Loss: 0.3942687249031005\n",
            " Batch 8000 | Loss: 0.37533525120004196\n",
            " Batch 9000 | Loss: 0.34664463033148785\n",
            " Batch 10000 | Loss: 0.35717637390567686\n",
            " Batch 11000 | Loss: 0.3535203938072082\n",
            " Batch 12000 | Loss: 0.36672245666524395\n",
            " Batch 13000 | Loss: 0.33650204522442073\n",
            " Batch 14000 | Loss: 0.35096204840554857\n",
            " Batch 15000 | Loss: 0.3449012267652433\n",
            "LOSS train 0.3449012267652433 | valid: 0.36337122321128845\n",
            "EPOCH: 3\n",
            " Batch 1000 | Loss: 0.33153541084530297\n",
            " Batch 2000 | Loss: 0.31879076796340816\n",
            " Batch 3000 | Loss: 0.33795869718585164\n",
            " Batch 4000 | Loss: 0.33665988868804564\n",
            " Batch 5000 | Loss: 0.3321101684459427\n",
            " Batch 6000 | Loss: 0.30912384321773423\n",
            " Batch 7000 | Loss: 0.3347054498351354\n",
            " Batch 8000 | Loss: 0.3127133726463799\n",
            " Batch 9000 | Loss: 0.31269004911181403\n",
            " Batch 10000 | Loss: 0.3147576666316891\n",
            " Batch 11000 | Loss: 0.3227961595086963\n",
            " Batch 12000 | Loss: 0.3074295733131221\n",
            " Batch 13000 | Loss: 0.32073379334333\n",
            " Batch 14000 | Loss: 0.3166891499254416\n",
            " Batch 15000 | Loss: 0.3290509850471426\n",
            "LOSS train 0.3290509850471426 | valid: 0.34634771943092346\n",
            "EPOCH: 4\n",
            " Batch 1000 | Loss: 0.3149891384478251\n",
            " Batch 2000 | Loss: 0.29068007103886384\n",
            " Batch 3000 | Loss: 0.2813695177410482\n",
            " Batch 4000 | Loss: 0.3173553416520736\n",
            " Batch 5000 | Loss: 0.2982783965052804\n",
            " Batch 6000 | Loss: 0.31100574785935714\n",
            " Batch 7000 | Loss: 0.2998503201607018\n",
            " Batch 8000 | Loss: 0.2972440680736763\n",
            " Batch 9000 | Loss: 0.3062436990353017\n",
            " Batch 10000 | Loss: 0.2946489991768321\n",
            " Batch 11000 | Loss: 0.30604351752291403\n",
            " Batch 12000 | Loss: 0.28744923849531917\n",
            " Batch 13000 | Loss: 0.28954303692754185\n",
            " Batch 14000 | Loss: 0.2856231758062495\n",
            " Batch 15000 | Loss: 0.2934723401251831\n",
            "LOSS train 0.2934723401251831 | valid: 0.3256673812866211\n",
            "EPOCH: 5\n",
            " Batch 1000 | Loss: 0.2753879572272126\n",
            " Batch 2000 | Loss: 0.28809454928820194\n",
            " Batch 3000 | Loss: 0.28372241899539224\n",
            " Batch 4000 | Loss: 0.27230459116748534\n",
            " Batch 5000 | Loss: 0.29161902536939305\n",
            " Batch 6000 | Loss: 0.2652373347202883\n",
            " Batch 7000 | Loss: 0.2657579893119364\n",
            " Batch 8000 | Loss: 0.2978963730534742\n",
            " Batch 9000 | Loss: 0.2709011787661875\n",
            " Batch 10000 | Loss: 0.2825320183834483\n",
            " Batch 11000 | Loss: 0.28739942337144747\n",
            " Batch 12000 | Loss: 0.2801504018845735\n",
            " Batch 13000 | Loss: 0.2763326338741663\n",
            " Batch 14000 | Loss: 0.2848514487763941\n",
            " Batch 15000 | Loss: 0.26825555233819975\n",
            "LOSS train 0.26825555233819975 | valid: 0.3133198022842407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, let's visualize the data as a sanity check:"
      ],
      "metadata": {
        "id": "pXI-1jRgxsl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Helper function for inline image display\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "  if one_channel:\n",
        "    img = img.mean(dim=0)  # less the color channel\n",
        "  img = img / 2 + 0.5  # unnormalize\n",
        "  npimg = img.numpy()\n",
        "  if one_channel:\n",
        "    plt.imshow(npimg, cmap=\"Greys\")\n",
        "  else:\n",
        "    plt.imshow(np.tranpose(npimg, (1,2,0)))\n",
        "\n",
        "dataiter = iter(training_dataloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Build a grid from the images and show them\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "matplotlib_imshow(img_grid, one_channel=True)\n",
        "\n",
        "print(' '.join(classes[labels[j]] for j in range(4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "4GfT89JUFhgl",
        "outputId": "38a9dc51-2f98-4a63-f7ef-87a9384308a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trouser Coat Sandal Sandal\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toload a saved version of the model:"
      ],
      "metadata": {
        "id": "6yQ6ObLdII6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = GarmentClassifier()\n",
        "saved_model.state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2D7niMyIkLw",
        "outputId": "f21b31d2-8bad-4c82-8a8b-523f236c090b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-36f90c080f5f>:2: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  saved_model.state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('conv1.weight',\n",
              "              tensor([[[[ 0.0127,  0.0763, -0.1238, -0.0374, -0.0447],\n",
              "                        [-0.0881,  0.1750,  0.1158,  0.1528, -0.1404],\n",
              "                        [ 0.1777, -0.1486, -0.1900,  0.0094, -0.0403],\n",
              "                        [-0.0427,  0.0150, -0.0553,  0.0527, -0.0295],\n",
              "                        [ 0.1168,  0.0217,  0.0441,  0.1796, -0.1982]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1816,  0.1895,  0.1782,  0.1311,  0.0604],\n",
              "                        [ 0.0948,  0.1949, -0.1013, -0.1404, -0.1491],\n",
              "                        [-0.0835,  0.1856, -0.1179,  0.1970,  0.1638],\n",
              "                        [-0.0363, -0.0230, -0.1870, -0.1822,  0.1267],\n",
              "                        [ 0.1142,  0.0481, -0.0018,  0.0493,  0.0620]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0185, -0.1802, -0.1275,  0.0772,  0.0513],\n",
              "                        [ 0.1041,  0.1925,  0.0580, -0.1581,  0.1476],\n",
              "                        [ 0.1258, -0.0786,  0.0178, -0.1261, -0.0629],\n",
              "                        [-0.1456, -0.0460, -0.1599,  0.0195,  0.1546],\n",
              "                        [ 0.1772,  0.0443,  0.1226, -0.1272, -0.0764]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0719, -0.1998,  0.0113,  0.1305,  0.0263],\n",
              "                        [-0.0439, -0.0863, -0.0112, -0.1732, -0.1340],\n",
              "                        [ 0.1644, -0.0561,  0.1863, -0.1007, -0.1807],\n",
              "                        [ 0.0272,  0.0833,  0.1279, -0.1485,  0.1175],\n",
              "                        [-0.0532, -0.0870,  0.1539,  0.1787,  0.1493]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0366,  0.1713,  0.1856,  0.1951,  0.1504],\n",
              "                        [-0.0846, -0.0899,  0.1392,  0.0259, -0.0996],\n",
              "                        [ 0.1748,  0.0449, -0.0903, -0.0983,  0.0159],\n",
              "                        [-0.1456, -0.0072,  0.1060,  0.0116, -0.1667],\n",
              "                        [ 0.0575,  0.0911,  0.0101,  0.0507, -0.0568]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1165, -0.0629,  0.0052,  0.1947,  0.0321],\n",
              "                        [ 0.1655, -0.0200, -0.0237, -0.0284,  0.1123],\n",
              "                        [-0.0626, -0.1711,  0.0532, -0.0401, -0.0316],\n",
              "                        [-0.1176,  0.1550, -0.0622, -0.0707,  0.0114],\n",
              "                        [-0.1597,  0.1537,  0.0423,  0.1332,  0.0858]]]])),\n",
              "             ('conv1.bias',\n",
              "              tensor([-0.1080, -0.1799, -0.0692, -0.0233,  0.1553,  0.0431])),\n",
              "             ('conv2.weight',\n",
              "              tensor([[[[-0.0407,  0.0277,  0.0644,  0.0651, -0.0417],\n",
              "                        [-0.0651, -0.0460,  0.0797, -0.0148,  0.0105],\n",
              "                        [ 0.0448, -0.0729, -0.0234,  0.0123,  0.0405],\n",
              "                        [ 0.0368, -0.0313,  0.0560, -0.0060, -0.0449],\n",
              "                        [ 0.0252,  0.0551, -0.0398, -0.0212, -0.0169]],\n",
              "              \n",
              "                       [[ 0.0411,  0.0375,  0.0248,  0.0524,  0.0339],\n",
              "                        [ 0.0529,  0.0488, -0.0637,  0.0502,  0.0128],\n",
              "                        [-0.0092,  0.0307, -0.0007, -0.0586, -0.0772],\n",
              "                        [ 0.0175, -0.0616, -0.0109, -0.0666, -0.0605],\n",
              "                        [-0.0260,  0.0474,  0.0750,  0.0572, -0.0295]],\n",
              "              \n",
              "                       [[-0.0698, -0.0131, -0.0721, -0.0433,  0.0314],\n",
              "                        [-0.0436, -0.0397, -0.0384, -0.0238, -0.0460],\n",
              "                        [-0.0658,  0.0491, -0.0359,  0.0100, -0.0253],\n",
              "                        [ 0.0087,  0.0186, -0.0666, -0.0638,  0.0711],\n",
              "                        [ 0.0567,  0.0456, -0.0214, -0.0431, -0.0033]],\n",
              "              \n",
              "                       [[-0.0673, -0.0389,  0.0540,  0.0806,  0.0419],\n",
              "                        [ 0.0244,  0.0267,  0.0269, -0.0477,  0.0470],\n",
              "                        [-0.0603, -0.0565, -0.0690,  0.0672, -0.0115],\n",
              "                        [ 0.0510, -0.0590, -0.0720, -0.0477,  0.0735],\n",
              "                        [ 0.0699,  0.0426,  0.0098, -0.0579, -0.0465]],\n",
              "              \n",
              "                       [[ 0.0442, -0.0489, -0.0061,  0.0610, -0.0813],\n",
              "                        [ 0.0026, -0.0779, -0.0641,  0.0810,  0.0807],\n",
              "                        [-0.0002,  0.0027, -0.0700, -0.0352, -0.0303],\n",
              "                        [ 0.0350,  0.0478, -0.0365,  0.0582,  0.0346],\n",
              "                        [-0.0588,  0.0174, -0.0131, -0.0230,  0.0356]],\n",
              "              \n",
              "                       [[ 0.0772, -0.0255, -0.0298,  0.0463, -0.0374],\n",
              "                        [ 0.0310,  0.0193, -0.0318,  0.0123, -0.0681],\n",
              "                        [ 0.0363, -0.0765, -0.0461,  0.0029,  0.0153],\n",
              "                        [ 0.0290,  0.0091, -0.0812,  0.0578,  0.0788],\n",
              "                        [-0.0177, -0.0784,  0.0396, -0.0514,  0.0315]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0253, -0.0509, -0.0648, -0.0349, -0.0337],\n",
              "                        [-0.0270,  0.0522,  0.0118, -0.0631, -0.0426],\n",
              "                        [ 0.0271,  0.0640,  0.0280, -0.0619,  0.0166],\n",
              "                        [-0.0615,  0.0764, -0.0454,  0.0164, -0.0486],\n",
              "                        [-0.0245,  0.0248, -0.0129, -0.0150, -0.0048]],\n",
              "              \n",
              "                       [[-0.0472,  0.0348,  0.0353,  0.0497,  0.0399],\n",
              "                        [ 0.0593, -0.0225, -0.0243,  0.0277,  0.0681],\n",
              "                        [-0.0449,  0.0109, -0.0229,  0.0539,  0.0212],\n",
              "                        [-0.0324,  0.0355, -0.0349,  0.0247,  0.0112],\n",
              "                        [ 0.0077,  0.0480,  0.0165, -0.0804, -0.0461]],\n",
              "              \n",
              "                       [[-0.0702, -0.0542, -0.0621, -0.0203,  0.0268],\n",
              "                        [ 0.0477,  0.0034,  0.0507,  0.0332, -0.0189],\n",
              "                        [-0.0046,  0.0359, -0.0254, -0.0716,  0.0507],\n",
              "                        [-0.0262, -0.0507,  0.0139,  0.0102,  0.0129],\n",
              "                        [-0.0437, -0.0382,  0.0111, -0.0054,  0.0369]],\n",
              "              \n",
              "                       [[-0.0134, -0.0457,  0.0316,  0.0037,  0.0424],\n",
              "                        [-0.0014,  0.0671, -0.0535,  0.0428,  0.0195],\n",
              "                        [-0.0731,  0.0124, -0.0461,  0.0738,  0.0104],\n",
              "                        [-0.0489,  0.0522, -0.0348,  0.0164,  0.0256],\n",
              "                        [ 0.0029, -0.0018, -0.0777,  0.0055,  0.0516]],\n",
              "              \n",
              "                       [[-0.0479,  0.0647, -0.0522,  0.0009, -0.0029],\n",
              "                        [-0.0363, -0.0784,  0.0378, -0.0030,  0.0111],\n",
              "                        [-0.0706,  0.0573,  0.0060,  0.0619, -0.0251],\n",
              "                        [-0.0539,  0.0137, -0.0129,  0.0250,  0.0362],\n",
              "                        [ 0.0080, -0.0126, -0.0054, -0.0669,  0.0197]],\n",
              "              \n",
              "                       [[-0.0290, -0.0025,  0.0111, -0.0667,  0.0291],\n",
              "                        [ 0.0513, -0.0698,  0.0121, -0.0789,  0.0503],\n",
              "                        [-0.0243, -0.0622,  0.0773, -0.0209,  0.0739],\n",
              "                        [ 0.0607, -0.0420,  0.0114, -0.0579, -0.0373],\n",
              "                        [-0.0058, -0.0549, -0.0042,  0.0133,  0.0373]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0455,  0.0583,  0.0173,  0.0342, -0.0239],\n",
              "                        [-0.0450,  0.0308,  0.0144,  0.0060, -0.0750],\n",
              "                        [-0.0340,  0.0356,  0.0009,  0.0803,  0.0377],\n",
              "                        [ 0.0756, -0.0469,  0.0657,  0.0816,  0.0452],\n",
              "                        [ 0.0025, -0.0145, -0.0527,  0.0336,  0.0283]],\n",
              "              \n",
              "                       [[ 0.0180, -0.0575, -0.0774,  0.0573,  0.0479],\n",
              "                        [-0.0135, -0.0265,  0.0522, -0.0118, -0.0651],\n",
              "                        [-0.0226, -0.0599, -0.0625,  0.0623, -0.0179],\n",
              "                        [ 0.0236,  0.0578,  0.0725, -0.0045,  0.0041],\n",
              "                        [ 0.0312, -0.0325, -0.0092,  0.0619,  0.0123]],\n",
              "              \n",
              "                       [[-0.0804,  0.0274,  0.0680,  0.0538, -0.0366],\n",
              "                        [-0.0208,  0.0201,  0.0456, -0.0582,  0.0406],\n",
              "                        [ 0.0733,  0.0745, -0.0214, -0.0757, -0.0367],\n",
              "                        [-0.0072,  0.0701,  0.0012, -0.0519,  0.0797],\n",
              "                        [-0.0430,  0.0515,  0.0744, -0.0736,  0.0658]],\n",
              "              \n",
              "                       [[ 0.0450, -0.0130, -0.0436, -0.0715, -0.0694],\n",
              "                        [ 0.0687, -0.0025, -0.0648,  0.0663,  0.0186],\n",
              "                        [-0.0578, -0.0495, -0.0051, -0.0255, -0.0146],\n",
              "                        [ 0.0033,  0.0157, -0.0445,  0.0386, -0.0167],\n",
              "                        [-0.0517, -0.0455,  0.0280, -0.0301, -0.0637]],\n",
              "              \n",
              "                       [[ 0.0010,  0.0684,  0.0405,  0.0266,  0.0362],\n",
              "                        [-0.0204,  0.0775, -0.0165,  0.0166, -0.0775],\n",
              "                        [-0.0421,  0.0205, -0.0174,  0.0067, -0.0055],\n",
              "                        [ 0.0628,  0.0557, -0.0085, -0.0263, -0.0447],\n",
              "                        [-0.0225, -0.0110,  0.0291,  0.0124, -0.0742]],\n",
              "              \n",
              "                       [[-0.0596, -0.0758,  0.0508, -0.0775,  0.0017],\n",
              "                        [ 0.0290,  0.0206,  0.0692,  0.0746, -0.0545],\n",
              "                        [-0.0160,  0.0737,  0.0401,  0.0766, -0.0416],\n",
              "                        [ 0.0464, -0.0321, -0.0038,  0.0265,  0.0420],\n",
              "                        [-0.0302, -0.0538, -0.0210, -0.0263,  0.0648]]],\n",
              "              \n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0674,  0.0678, -0.0164,  0.0405,  0.0790],\n",
              "                        [ 0.0453, -0.0099,  0.0634,  0.0175, -0.0021],\n",
              "                        [ 0.0435,  0.0115,  0.0801, -0.0576,  0.0578],\n",
              "                        [-0.0426,  0.0271,  0.0086, -0.0407, -0.0027],\n",
              "                        [ 0.0336, -0.0495,  0.0411,  0.0390,  0.0226]],\n",
              "              \n",
              "                       [[ 0.0631, -0.0075, -0.0357,  0.0803,  0.0114],\n",
              "                        [ 0.0639,  0.0672, -0.0677, -0.0058, -0.0526],\n",
              "                        [ 0.0339, -0.0455, -0.0367,  0.0773, -0.0286],\n",
              "                        [ 0.0384,  0.0344,  0.0602, -0.0073,  0.0778],\n",
              "                        [ 0.0040, -0.0281,  0.0572, -0.0716,  0.0537]],\n",
              "              \n",
              "                       [[-0.0165, -0.0054, -0.0221,  0.0373, -0.0104],\n",
              "                        [-0.0292, -0.0071,  0.0585,  0.0223, -0.0780],\n",
              "                        [-0.0667,  0.0210,  0.0793,  0.0502, -0.0687],\n",
              "                        [ 0.0694, -0.0522, -0.0579,  0.0612, -0.0492],\n",
              "                        [ 0.0801, -0.0452, -0.0451,  0.0663,  0.0113]],\n",
              "              \n",
              "                       [[ 0.0739,  0.0638,  0.0692, -0.0672,  0.0155],\n",
              "                        [-0.0638,  0.0748,  0.0116, -0.0146,  0.0639],\n",
              "                        [ 0.0244,  0.0190,  0.0102,  0.0106, -0.0187],\n",
              "                        [ 0.0557, -0.0218, -0.0065,  0.0506, -0.0738],\n",
              "                        [-0.0479, -0.0507,  0.0367, -0.0067,  0.0208]],\n",
              "              \n",
              "                       [[ 0.0677, -0.0654, -0.0456,  0.0349,  0.0449],\n",
              "                        [ 0.0463, -0.0125,  0.0563,  0.0341, -0.0585],\n",
              "                        [-0.0055, -0.0227,  0.0545, -0.0110,  0.0723],\n",
              "                        [-0.0021,  0.0122,  0.0268,  0.0038,  0.0351],\n",
              "                        [-0.0040,  0.0703, -0.0062, -0.0268, -0.0340]],\n",
              "              \n",
              "                       [[-0.0321, -0.0374,  0.0455,  0.0077, -0.0455],\n",
              "                        [ 0.0171, -0.0654, -0.0516,  0.0145, -0.0546],\n",
              "                        [ 0.0526, -0.0791,  0.0618, -0.0103,  0.0395],\n",
              "                        [-0.0740,  0.0441,  0.0295,  0.0052,  0.0598],\n",
              "                        [ 0.0009,  0.0271, -0.0175, -0.0045,  0.0188]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0623, -0.0303, -0.0472,  0.0621, -0.0729],\n",
              "                        [-0.0307, -0.0225,  0.0691, -0.0328, -0.0088],\n",
              "                        [-0.0182, -0.0362, -0.0448,  0.0266,  0.0227],\n",
              "                        [-0.0630,  0.0454,  0.0200,  0.0472, -0.0641],\n",
              "                        [-0.0375, -0.0247, -0.0605, -0.0354, -0.0337]],\n",
              "              \n",
              "                       [[-0.0378, -0.0132,  0.0659, -0.0033,  0.0044],\n",
              "                        [-0.0332, -0.0425,  0.0468,  0.0706,  0.0262],\n",
              "                        [ 0.0391,  0.0516,  0.0490, -0.0230,  0.0613],\n",
              "                        [-0.0568, -0.0420, -0.0231, -0.0520,  0.0447],\n",
              "                        [-0.0426,  0.0046,  0.0132,  0.0013,  0.0016]],\n",
              "              \n",
              "                       [[-0.0716,  0.0388,  0.0742, -0.0225, -0.0701],\n",
              "                        [ 0.0655,  0.0399, -0.0192, -0.0326, -0.0258],\n",
              "                        [-0.0497, -0.0530,  0.0815,  0.0392,  0.0173],\n",
              "                        [ 0.0234,  0.0540, -0.0221, -0.0206,  0.0586],\n",
              "                        [ 0.0680,  0.0056, -0.0308,  0.0207, -0.0073]],\n",
              "              \n",
              "                       [[-0.0619,  0.0486, -0.0017,  0.0732,  0.0287],\n",
              "                        [-0.0117,  0.0168,  0.0111,  0.0116, -0.0015],\n",
              "                        [ 0.0510, -0.0433,  0.0803,  0.0607,  0.0064],\n",
              "                        [ 0.0538,  0.0188, -0.0415, -0.0455,  0.0117],\n",
              "                        [ 0.0594, -0.0556,  0.0711, -0.0769, -0.0698]],\n",
              "              \n",
              "                       [[-0.0246,  0.0353,  0.0558, -0.0434, -0.0203],\n",
              "                        [-0.0488,  0.0142, -0.0534,  0.0088, -0.0172],\n",
              "                        [ 0.0372,  0.0201, -0.0373,  0.0344,  0.0533],\n",
              "                        [-0.0239, -0.0140,  0.0727,  0.0512,  0.0762],\n",
              "                        [-0.0592,  0.0644,  0.0210, -0.0715,  0.0710]],\n",
              "              \n",
              "                       [[ 0.0534,  0.0537,  0.0757, -0.0704,  0.0182],\n",
              "                        [-0.0477,  0.0020, -0.0611, -0.0105,  0.0159],\n",
              "                        [ 0.0080,  0.0363,  0.0282,  0.0032,  0.0530],\n",
              "                        [-0.0104, -0.0064,  0.0415,  0.0169,  0.0110],\n",
              "                        [ 0.0050, -0.0731, -0.0091,  0.0222,  0.0039]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0338, -0.0726, -0.0433,  0.0159, -0.0651],\n",
              "                        [ 0.0570,  0.0243,  0.0489, -0.0096,  0.0237],\n",
              "                        [-0.0024, -0.0752, -0.0745, -0.0262, -0.0283],\n",
              "                        [ 0.0689, -0.0459,  0.0332, -0.0763,  0.0308],\n",
              "                        [-0.0784,  0.0315, -0.0737, -0.0232,  0.0505]],\n",
              "              \n",
              "                       [[-0.0744,  0.0189, -0.0136,  0.0316,  0.0433],\n",
              "                        [-0.0632, -0.0038,  0.0483, -0.0380,  0.0092],\n",
              "                        [ 0.0790,  0.0120,  0.0648,  0.0050,  0.0251],\n",
              "                        [ 0.0654, -0.0533,  0.0196,  0.0614, -0.0199],\n",
              "                        [-0.0116, -0.0762,  0.0391,  0.0090,  0.0574]],\n",
              "              \n",
              "                       [[-0.0620, -0.0812,  0.0137,  0.0009, -0.0558],\n",
              "                        [-0.0574, -0.0119,  0.0728,  0.0079, -0.0431],\n",
              "                        [ 0.0395, -0.0262, -0.0073,  0.0698, -0.0028],\n",
              "                        [ 0.0675, -0.0490,  0.0012, -0.0133,  0.0614],\n",
              "                        [-0.0757, -0.0077,  0.0231, -0.0812,  0.0244]],\n",
              "              \n",
              "                       [[ 0.0735, -0.0122, -0.0228, -0.0281,  0.0662],\n",
              "                        [-0.0812,  0.0807, -0.0441,  0.0111, -0.0032],\n",
              "                        [-0.0026,  0.0216,  0.0250, -0.0234, -0.0569],\n",
              "                        [-0.0244, -0.0804,  0.0466, -0.0357, -0.0470],\n",
              "                        [-0.0634,  0.0128,  0.0588,  0.0142,  0.0644]],\n",
              "              \n",
              "                       [[-0.0315,  0.0324,  0.0785, -0.0560, -0.0795],\n",
              "                        [ 0.0149, -0.0079,  0.0630, -0.0545, -0.0698],\n",
              "                        [-0.0486, -0.0216,  0.0059, -0.0134,  0.0264],\n",
              "                        [ 0.0517, -0.0791, -0.0538, -0.0816,  0.0113],\n",
              "                        [ 0.0366,  0.0733, -0.0225,  0.0410, -0.0547]],\n",
              "              \n",
              "                       [[-0.0684, -0.0440,  0.0239, -0.0070, -0.0560],\n",
              "                        [ 0.0030,  0.0354,  0.0287,  0.0343, -0.0399],\n",
              "                        [-0.0747,  0.0181,  0.0105,  0.0386, -0.0076],\n",
              "                        [-0.0473,  0.0485, -0.0680,  0.0008,  0.0148],\n",
              "                        [-0.0649,  0.0626,  0.0740,  0.0780, -0.0766]]]])),\n",
              "             ('conv2.bias',\n",
              "              tensor([-0.0570,  0.0250, -0.0554, -0.0558, -0.0222,  0.0532, -0.0268,  0.0203,\n",
              "                      -0.0178, -0.0398, -0.0717,  0.0248,  0.0451,  0.0499,  0.0671, -0.0663])),\n",
              "             ('fc1.weight',\n",
              "              tensor([[-0.0382, -0.0160, -0.0067,  ...,  0.0321,  0.0482,  0.0115],\n",
              "                      [ 0.0073, -0.0198,  0.0191,  ..., -0.0391,  0.0082, -0.0500],\n",
              "                      [-0.0321, -0.0405, -0.0266,  ..., -0.0393, -0.0411,  0.0013],\n",
              "                      ...,\n",
              "                      [ 0.0008, -0.0399, -0.0139,  ..., -0.0604,  0.0024,  0.0345],\n",
              "                      [ 0.0303, -0.0437, -0.0275,  ...,  0.0516,  0.0591, -0.0468],\n",
              "                      [-0.0265,  0.0170, -0.0387,  ..., -0.0209,  0.0140, -0.0071]])),\n",
              "             ('fc1.bias',\n",
              "              tensor([-0.0225,  0.0566, -0.0016,  0.0358, -0.0146,  0.0026,  0.0488, -0.0282,\n",
              "                       0.0084,  0.0296,  0.0215,  0.0252,  0.0472,  0.0212,  0.0257, -0.0156,\n",
              "                       0.0615, -0.0542,  0.0239, -0.0285,  0.0581, -0.0520, -0.0466, -0.0468,\n",
              "                      -0.0409,  0.0084,  0.0415,  0.0569, -0.0425, -0.0475,  0.0089,  0.0505,\n",
              "                      -0.0259, -0.0205,  0.0159,  0.0404,  0.0283,  0.0579,  0.0619, -0.0602,\n",
              "                      -0.0289, -0.0339, -0.0048, -0.0441,  0.0316, -0.0310,  0.0599, -0.0133,\n",
              "                      -0.0464, -0.0172,  0.0510, -0.0601,  0.0245, -0.0040, -0.0129,  0.0033,\n",
              "                      -0.0447, -0.0497,  0.0067, -0.0023, -0.0268,  0.0023, -0.0147, -0.0334,\n",
              "                       0.0255, -0.0102, -0.0497, -0.0384, -0.0490,  0.0595, -0.0036,  0.0269,\n",
              "                      -0.0361,  0.0329,  0.0053,  0.0083, -0.0397,  0.0347, -0.0345, -0.0445,\n",
              "                       0.0451, -0.0403, -0.0041,  0.0010, -0.0212, -0.0120, -0.0160, -0.0091,\n",
              "                       0.0436, -0.0484,  0.0596,  0.0580, -0.0504, -0.0120, -0.0446,  0.0267,\n",
              "                       0.0095, -0.0457, -0.0479, -0.0113, -0.0426,  0.0572,  0.0413,  0.0045,\n",
              "                      -0.0376, -0.0380, -0.0225, -0.0563,  0.0080, -0.0161, -0.0280,  0.0422,\n",
              "                       0.0605,  0.0153,  0.0024, -0.0407, -0.0420, -0.0222,  0.0289,  0.0379])),\n",
              "             ('fc2.weight',\n",
              "              tensor([[-0.0035, -0.0478,  0.0304,  ...,  0.0691, -0.0428,  0.0063],\n",
              "                      [ 0.0482,  0.0498, -0.0546,  ..., -0.0805,  0.0237,  0.0047],\n",
              "                      [ 0.0505,  0.0722, -0.0534,  ...,  0.0168, -0.0908, -0.0417],\n",
              "                      ...,\n",
              "                      [ 0.0300, -0.0179,  0.0005,  ...,  0.0279, -0.0049,  0.0615],\n",
              "                      [-0.0709,  0.0441,  0.0758,  ...,  0.0160,  0.0591,  0.0073],\n",
              "                      [-0.0483,  0.0310,  0.0070,  ..., -0.0409, -0.0701,  0.0109]])),\n",
              "             ('fc2.bias',\n",
              "              tensor([ 0.0061, -0.0831, -0.0685,  0.0877, -0.0369, -0.0571,  0.0852, -0.0512,\n",
              "                       0.0275,  0.0745, -0.0181,  0.0084,  0.0540,  0.0496,  0.0872,  0.0721,\n",
              "                       0.0710, -0.0425,  0.0253,  0.0730, -0.0425,  0.0419,  0.0185,  0.0503,\n",
              "                       0.0815,  0.0891,  0.0456, -0.0026, -0.0417,  0.0106, -0.0209,  0.0291,\n",
              "                       0.0415, -0.0143,  0.0061,  0.0761, -0.0353,  0.0190, -0.0628,  0.0804,\n",
              "                      -0.0045,  0.0658, -0.0581,  0.0222, -0.0469, -0.0909,  0.0355, -0.0043,\n",
              "                      -0.0319,  0.0678, -0.0629, -0.0186, -0.0554, -0.0093, -0.0766,  0.0340,\n",
              "                      -0.0443, -0.0718, -0.0811, -0.0323,  0.0887,  0.0153,  0.0420,  0.0184,\n",
              "                       0.0868, -0.0504,  0.0627, -0.0018, -0.0547,  0.0605, -0.0656,  0.0499,\n",
              "                       0.0250,  0.0910,  0.0310,  0.0844, -0.0468, -0.0513,  0.0851,  0.0084,\n",
              "                       0.0114,  0.0873,  0.0411,  0.0909])),\n",
              "             ('fc3.weight',\n",
              "              tensor([[ 0.0672, -0.0950,  0.0262, -0.0188,  0.0279,  0.0433, -0.0816,  0.1008,\n",
              "                       -0.0654, -0.1047, -0.0359, -0.0303,  0.0351, -0.0301,  0.1024,  0.0437,\n",
              "                       -0.0298,  0.0056,  0.0807,  0.0864, -0.0371, -0.0912, -0.0900, -0.0784,\n",
              "                       -0.0150, -0.0554, -0.0437,  0.0105,  0.0131, -0.0001,  0.0668,  0.0268,\n",
              "                       -0.0249, -0.0177,  0.1033, -0.0836,  0.0509, -0.0253,  0.0278,  0.0352,\n",
              "                        0.0895,  0.0948, -0.0813,  0.0380,  0.0456,  0.0524, -0.0497, -0.0194,\n",
              "                        0.0218,  0.0187,  0.1041, -0.0211, -0.0632, -0.1039,  0.1021, -0.0575,\n",
              "                       -0.0006, -0.0800, -0.0598,  0.0844, -0.0691, -0.0352, -0.0517,  0.0186,\n",
              "                       -0.0477, -0.0513, -0.0540,  0.0031, -0.0961,  0.0972,  0.0019, -0.0639,\n",
              "                       -0.0656,  0.0279, -0.0480, -0.0868,  0.1061,  0.0332, -0.0569,  0.0743,\n",
              "                        0.0531,  0.0391,  0.0722,  0.0495],\n",
              "                      [-0.0315, -0.0684,  0.0922,  0.0418,  0.0501, -0.0487,  0.0081,  0.0266,\n",
              "                       -0.0737,  0.0584, -0.0415,  0.0725, -0.0092,  0.0476, -0.0608, -0.0458,\n",
              "                       -0.0120,  0.0842, -0.0364, -0.0422,  0.0941, -0.0352, -0.0373,  0.0407,\n",
              "                        0.1075,  0.0533,  0.0976, -0.0739, -0.0135, -0.0959, -0.1022,  0.0703,\n",
              "                       -0.0369, -0.0446, -0.0732,  0.0701, -0.0155, -0.0269, -0.0188,  0.0294,\n",
              "                       -0.0460,  0.0099, -0.0651, -0.0533, -0.0902, -0.0758,  0.0810, -0.0059,\n",
              "                       -0.1005,  0.1018,  0.0875, -0.0534, -0.0626,  0.0729,  0.0428, -0.0315,\n",
              "                        0.0618,  0.0205, -0.0856,  0.0589, -0.0539,  0.0747, -0.0508, -0.0876,\n",
              "                       -0.0113,  0.0864, -0.0863, -0.0668, -0.0614,  0.0397, -0.0348,  0.1015,\n",
              "                       -0.1011,  0.0178, -0.0254,  0.0936, -0.0648, -0.0477, -0.0561,  0.0597,\n",
              "                       -0.0062, -0.0037, -0.1042, -0.1018],\n",
              "                      [ 0.0164,  0.0276, -0.0978,  0.1085,  0.0809,  0.1049,  0.0187,  0.0552,\n",
              "                       -0.0121,  0.0560, -0.0667, -0.0728, -0.1084, -0.0534, -0.0936, -0.0341,\n",
              "                       -0.0015, -0.0256,  0.0120,  0.0471, -0.0843,  0.0495,  0.0434, -0.0922,\n",
              "                       -0.0423, -0.0149,  0.0305,  0.0931,  0.0031, -0.0176,  0.0315,  0.0844,\n",
              "                        0.0406,  0.0405, -0.0790, -0.0676, -0.0200, -0.0280,  0.0154, -0.0072,\n",
              "                        0.0407, -0.0306, -0.0866, -0.0591, -0.1086,  0.0438,  0.0601, -0.1057,\n",
              "                       -0.0434, -0.0696, -0.0683, -0.0502,  0.0688,  0.0588, -0.0028, -0.0389,\n",
              "                       -0.0784,  0.0480, -0.0898,  0.0205,  0.1077,  0.1085, -0.0233,  0.0708,\n",
              "                        0.0139, -0.0983, -0.0378,  0.0840,  0.0060, -0.0154, -0.0119, -0.0273,\n",
              "                       -0.0856,  0.1064, -0.0498,  0.0466,  0.0918, -0.0867,  0.0088,  0.0744,\n",
              "                        0.0821, -0.0770, -0.0904, -0.0765],\n",
              "                      [-0.0926,  0.0531, -0.0860, -0.0396,  0.0029, -0.1062, -0.0532, -0.0833,\n",
              "                       -0.0615, -0.0127, -0.0409, -0.1082, -0.0173, -0.0922,  0.0021, -0.0966,\n",
              "                        0.1055, -0.1047,  0.0995,  0.0395,  0.0289, -0.1017, -0.0634,  0.0422,\n",
              "                        0.0631, -0.0162, -0.0929,  0.0856,  0.0018,  0.0542, -0.0980, -0.0108,\n",
              "                        0.0506, -0.0058, -0.0645,  0.0380, -0.0152,  0.0556, -0.0797, -0.0655,\n",
              "                        0.0962, -0.0502, -0.0529, -0.0759, -0.0602, -0.0392,  0.1075,  0.0333,\n",
              "                        0.0041,  0.0132, -0.0015, -0.0943,  0.0942, -0.0862, -0.0523, -0.0994,\n",
              "                        0.0781, -0.0676,  0.1088,  0.1082,  0.0415, -0.0005, -0.0177,  0.0957,\n",
              "                       -0.0247, -0.0352, -0.1063, -0.0171, -0.0129,  0.0396,  0.0512,  0.0940,\n",
              "                       -0.0419,  0.0641,  0.1034,  0.0890,  0.0019,  0.0388, -0.0813,  0.0478,\n",
              "                       -0.0512,  0.0932,  0.0122,  0.0734],\n",
              "                      [ 0.1003, -0.0333,  0.0971,  0.0414, -0.0561, -0.0205, -0.0818, -0.0870,\n",
              "                       -0.0251, -0.0841, -0.0487, -0.0429,  0.0431,  0.0679, -0.0491,  0.0157,\n",
              "                        0.0780,  0.0577,  0.0575,  0.0690,  0.0894, -0.1057, -0.0221, -0.0469,\n",
              "                        0.0491,  0.1034,  0.0178,  0.0990, -0.0601,  0.0377,  0.0379, -0.0806,\n",
              "                       -0.0133, -0.0930,  0.0201,  0.0027, -0.0721, -0.0199,  0.0789, -0.0836,\n",
              "                       -0.0709, -0.1052, -0.0735,  0.0131, -0.0616, -0.0888, -0.0847, -0.0816,\n",
              "                       -0.0707, -0.0554,  0.0078,  0.0790, -0.0349, -0.0517,  0.0549,  0.0425,\n",
              "                       -0.0688, -0.0593,  0.1015,  0.0446,  0.0604,  0.0379, -0.0482,  0.1040,\n",
              "                       -0.0331,  0.0877,  0.0298, -0.0021, -0.0285, -0.0886,  0.0898, -0.0127,\n",
              "                        0.0592, -0.1053,  0.0608, -0.0531, -0.0999, -0.0863,  0.0314,  0.0989,\n",
              "                       -0.0837, -0.0624, -0.0100, -0.0432],\n",
              "                      [-0.0254,  0.0569,  0.0687,  0.0210, -0.0619,  0.0547,  0.0724,  0.0616,\n",
              "                        0.0502, -0.0180, -0.0750,  0.0359,  0.0142, -0.0381,  0.0899, -0.0417,\n",
              "                        0.0476, -0.0499, -0.0672, -0.0163, -0.0548,  0.0498, -0.0842,  0.0792,\n",
              "                       -0.1016, -0.1054, -0.1045, -0.0235,  0.0467, -0.0065,  0.1025, -0.0631,\n",
              "                       -0.0922,  0.1079,  0.0200, -0.0672,  0.0161,  0.1015, -0.0007,  0.0853,\n",
              "                       -0.0456,  0.0172,  0.0790,  0.0872,  0.0572, -0.0802,  0.0786,  0.0554,\n",
              "                        0.0744, -0.0135,  0.0618, -0.0036, -0.0204, -0.0140,  0.0069,  0.0467,\n",
              "                       -0.0720,  0.0333,  0.0991, -0.0243, -0.0175, -0.0086,  0.1000,  0.0919,\n",
              "                        0.0927, -0.0762,  0.0408,  0.0063, -0.0254,  0.0532, -0.0214, -0.0292,\n",
              "                        0.0865, -0.0243, -0.0328, -0.0099,  0.0036, -0.0450, -0.1050,  0.0678,\n",
              "                       -0.0363, -0.0806, -0.0572, -0.0369],\n",
              "                      [ 0.0983,  0.0779, -0.0517, -0.0834, -0.0138, -0.0815,  0.0859, -0.0611,\n",
              "                        0.0662,  0.0464, -0.0790, -0.0115,  0.0187, -0.0953,  0.0865, -0.0914,\n",
              "                        0.1064, -0.0309, -0.1048,  0.0817,  0.0198, -0.0873,  0.0354,  0.1037,\n",
              "                        0.0564,  0.0765, -0.0614,  0.0219, -0.0682, -0.0953,  0.0110,  0.0870,\n",
              "                       -0.1088, -0.0732, -0.0146,  0.0756, -0.1008,  0.0711,  0.0304,  0.0100,\n",
              "                        0.0467,  0.0417, -0.0247, -0.0287, -0.0463,  0.0519, -0.0692, -0.0948,\n",
              "                       -0.0013,  0.0265, -0.0312, -0.0061,  0.0889,  0.0174,  0.0053,  0.0111,\n",
              "                        0.0166, -0.0463,  0.0861,  0.1033,  0.0384,  0.0982,  0.0533,  0.1053,\n",
              "                        0.0449, -0.0572,  0.1016, -0.0186,  0.0120,  0.0709, -0.0276,  0.0804,\n",
              "                        0.0745, -0.0960, -0.0831,  0.0937,  0.0850,  0.0665, -0.1081, -0.0770,\n",
              "                       -0.0020,  0.0932,  0.0009, -0.0001],\n",
              "                      [-0.1022,  0.0811, -0.0478, -0.0504,  0.0599,  0.0447, -0.0285,  0.0708,\n",
              "                        0.0444,  0.0914, -0.0302,  0.0148, -0.0174,  0.0567, -0.0599, -0.0431,\n",
              "                        0.1005,  0.0554,  0.0850,  0.0613, -0.0691, -0.0412,  0.0968,  0.0145,\n",
              "                        0.0460,  0.0510, -0.0084, -0.1010, -0.0628,  0.0959,  0.0853, -0.0558,\n",
              "                        0.0474,  0.0363, -0.0784, -0.0315, -0.0814, -0.0947,  0.0745, -0.0663,\n",
              "                       -0.0521,  0.0994, -0.0559, -0.0442, -0.0296,  0.1083, -0.0250,  0.0538,\n",
              "                       -0.0508,  0.0381, -0.0245,  0.0293, -0.0040,  0.0474,  0.0698,  0.0689,\n",
              "                       -0.0493, -0.0246,  0.0439,  0.0598,  0.0761,  0.0745,  0.0224, -0.0141,\n",
              "                        0.0249,  0.0227,  0.0270,  0.0795,  0.0989,  0.0846,  0.0898, -0.0070,\n",
              "                        0.0286,  0.0400, -0.0314, -0.0128, -0.0676,  0.0679, -0.0375, -0.0691,\n",
              "                       -0.0228, -0.0959,  0.0113, -0.0235],\n",
              "                      [ 0.0872,  0.0540,  0.0226,  0.0836,  0.0930,  0.0681,  0.0400, -0.0686,\n",
              "                       -0.0683, -0.0529, -0.0903, -0.0574,  0.0503,  0.0565,  0.0893, -0.1062,\n",
              "                        0.0045, -0.0224,  0.0067,  0.0511, -0.0310,  0.0725,  0.0449,  0.0199,\n",
              "                        0.0335,  0.0985, -0.0362, -0.0668,  0.0478,  0.1041,  0.0734, -0.0789,\n",
              "                       -0.0910,  0.0270, -0.1010,  0.0064, -0.0505,  0.0493,  0.0340, -0.0734,\n",
              "                       -0.0364,  0.0467,  0.0740,  0.1027, -0.1061,  0.0405, -0.1086,  0.0979,\n",
              "                       -0.0701,  0.0588,  0.0203, -0.0656,  0.0430,  0.0536,  0.0205,  0.0881,\n",
              "                       -0.0629, -0.0110, -0.0026, -0.0365,  0.1054,  0.0878, -0.0944, -0.0021,\n",
              "                        0.1002,  0.0293,  0.0785,  0.0714, -0.0940,  0.0636, -0.0352,  0.0261,\n",
              "                        0.0233, -0.0749,  0.0913, -0.0994, -0.0381,  0.0284,  0.0644, -0.0056,\n",
              "                       -0.0144,  0.0991, -0.0555,  0.0956],\n",
              "                      [-0.1084, -0.0356,  0.0006, -0.0389, -0.0431,  0.0548, -0.0309,  0.0229,\n",
              "                       -0.0812,  0.0667,  0.0913,  0.0999,  0.0663, -0.0297, -0.0243,  0.1074,\n",
              "                        0.1046, -0.0231, -0.0985,  0.0511, -0.0122, -0.0709,  0.0557, -0.0858,\n",
              "                       -0.1090, -0.0015,  0.0908, -0.0844,  0.0259, -0.0067, -0.0233,  0.0100,\n",
              "                        0.0266,  0.0776, -0.0526,  0.0141,  0.0425, -0.1064, -0.0915, -0.0341,\n",
              "                       -0.0569,  0.0949, -0.0435, -0.0614,  0.0102, -0.0155,  0.0380,  0.0639,\n",
              "                       -0.0913, -0.0100,  0.1060, -0.0672, -0.0282, -0.0098,  0.1075,  0.0450,\n",
              "                        0.0567,  0.0556, -0.0862, -0.0056,  0.0532, -0.0267,  0.0826, -0.0634,\n",
              "                        0.0168, -0.0192, -0.0789, -0.0949, -0.1067,  0.0554, -0.0598,  0.0397,\n",
              "                        0.0088, -0.0356, -0.0381,  0.0709, -0.0527, -0.0898, -0.0197,  0.0731,\n",
              "                       -0.0996,  0.0660, -0.0001,  0.0846]])),\n",
              "             ('fc3.bias',\n",
              "              tensor([ 0.0626, -0.0325, -0.1059, -0.0775, -0.0835,  0.0786, -0.0154, -0.0529,\n",
              "                      -0.0512,  0.0060]))])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CdtUSWHd8thn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}