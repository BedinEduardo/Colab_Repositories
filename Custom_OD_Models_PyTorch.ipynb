{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BedinEduardo/Colab_Repositories/blob/master/Custom_OD_Models_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original code: [here](https://medium.com/@noel.benji/customizing-object-detection-models-with-lightweight-pytorch-code-ed043e48a460#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImJhNjNiNDM2ODM2YTkzOWI3OTViNDEyMmQzZjRkMGQyMjVkMWM3MDAiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDEyMTk5NDY0MDczOTg5MjY4NjciLCJlbWFpbCI6ImVkdWFyZG9iZWRpbjg5QGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJuYmYiOjE3NTQ1Nzc3MzMsIm5hbWUiOiJFZHVhcmRvIEJlZGluIiwiZ2l2ZW5fbmFtZSI6IkVkdWFyZG8iLCJmYW1pbHlfbmFtZSI6IkJlZGluIiwiaWF0IjoxNzU0NTc4MDMzLCJleHAiOjE3NTQ1ODE2MzMsImp0aSI6IjU3MWQ5YTUwNWZjMjM5NmE4YzA0M2NhMTUwMmZlN2FhZGE3NDQwNWMifQ.UiSRYTurLuCMTXHMdR6qYwwgIWo1zWNho5WtFzWAn3sTIeUIfnjgMtZnm-XrR7xE37VteK4vrw9Pl201T2QrnRcJmUReU6rbv6VNcJhswc4m3JdBn6lVQmoIUhgSrBg68drlp_uNo-lkKufvv6Yo_zD2bNjmxpYYvmCYo_CtX54FUuw7d900bQXpGDi5szSHCz5F8MJajVKSYWdD3lmTm49mZNSufBQxjdfiuOoQD-RlGnxcmO6c4I1o3qB0T_GyQS76hfPwH7kAYhmhHJtOY9JZ9aCIuIHtkaaXhjS13yh6-phJc5lMUBby9Oo5y-DWB4WD9zRvOt0oLo9t5Z_qJA)"
      ],
      "metadata": {
        "id": "JY2gPAz3qujM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHKMRz2Q64Xv"
      },
      "source": [
        "# Customizing Object Detection Models with Lightweight PyTorch Code\n",
        "\n",
        "Object Detection(OD) and segmentation are vital tasks in CV that aim to identify and localize objects in images. OD models predict bounding boxes and classify objects within them, while segmentation models go further by assigning a class label to each pixel in an image. These technologies power applications ranging from autonomous vehicles and facial recognition to medical imaging industrial automation.\n",
        "\n",
        "As the need for more precise, efficient, and adaptable models grows, there is increasing interest in lightweight models, customizable OD implementations. Many off-the-shelf solutions provide robust capabilities but often come with unnecessary overhead - training scripts, data loaders, augmentation pipelines, and prebuilt metrics - that can limit flexibility. For researchers and engineers who prefer building and controlling these components themselves, starting with barebones models offers a cleaner slate.\n",
        "\n",
        "PyTorch is a higly versatily deep learning framework preferred by many for its flexibility and developer-friendly design. It enables easy experimentation and customization, making it an ideal choice for developing OD models from scratch.\n",
        "Its dynamic computation graph and extensive library support ensure minimalist implementations remain powerful and extensible.\n",
        "\n",
        "As part of this tutorial, we will explore the process of building and customizing OD models using lightweight PyTorch code.\n",
        "From understanding core concepts to implementing backbones, detection heads, and forward passes, we will dive deep into the fundamentasl.\n",
        "\n",
        "## Core Concepts in OD and Segmentation\n",
        "\n",
        "OD and Segmentation models are composed of distinct components that work in unison to detect and classify objects within an image.\n",
        "Understanding these components and their interactions is critical for customizing lightweight implementations.\n",
        "\n",
        "### **Key Components of OD Models**\n",
        "\n",
        "**Backbone**: The backbone is a feature extraction network that performs the input image into a set of high-level feature maps.\n",
        "Common backbones include pretrained CNN like ResNet, MobileNet, EfficientNet and so one.\n",
        "These backbones reduce spatial dimensions while capturing semantic information.\n",
        "\n",
        "**Neck**: The neck process feature maps from the Backbone to enhance feature representation.\n",
        "Architectures like Feature Pyramid Network - FPN - and PANet are commonly used to aggregate features at different scales, making the model better at detecting objects at different scales.\n",
        "\n",
        "**Head**: The detection head generates the final outputs, such as bounding boxes, class labels, and confidence scores.\n",
        "Depending on the model, this can involve\n",
        "\n",
        "**Anchor-Based Heads**: Use predefined anchor boxes - Faster R-CNN\n",
        "**Anchor-Free Heads**: Predict object centers directly - YOLOv4, CenterNet\n",
        "\n",
        "### **Common Architectures**\n",
        "\n",
        "**Faster R-CNN**: Combines a CNN backbone with a Region Proposal Network - RPN - to generate candidate object regions. The proposals are refined in subsequent stages using bouding box regression and classification.\n",
        "\n",
        "**YOLO - You Only Look Once -**: A one-stage detector that divides the image into a grid and predicts bounding boxes and class probabilities directly.\n",
        "Its speed and simplicity make it popular for real-time applications.\n",
        "\n",
        "**Mask R-CNN**: Extends Faaster R-CNN by adding a parallel branch for instance segmentation, predicting masks for each detected object.\n",
        "\n",
        "### **Essential OD Techiniques**:\n",
        "\n",
        "**Anchor Generation**: Anchor boxes are predefined bounding boxes of various sizes and aspect ratios, used to match ground-truth objects. They enable the model to predict objects at different scales and locations.\n",
        "\n",
        "**Region Proposals**: Used in two-stage detectors like Faster R-CNN, the RPN generates regions of interest - ROIs - that are likely to contain objects.\n",
        "\n",
        "**Bounding Box Regression**: Bounding box regression adjust the anchor boxes or predictions to fit the object more precisely.\n",
        "This step ensures accurate localization.\n",
        "\n",
        "\n",
        "## Why Start with  Barebones Code?\n",
        "\n",
        "Starting with barebones codes in CV projects, particularly in tasks like OD or image segmentation, provides a foundation that foster deeper learning and flexibility.\n",
        "While high-level libraries like Detectron2 or MMDetection offer pre-built solutions, minimalist implementations empower developers to understand the inner workings of the model pipeline.\n",
        "\n",
        "### Benefits of Minimalist Implementations\n",
        "\n",
        "**Deeper Learning and Conceptual Clarity**: Barebones coding forces you to implement crictical components, such as data loaders, augmentation pipelines, and evaluation metrics, from scratch. This hands-on approach clarifies how these components interact, why ceratain designs are chosen, and the trade-offs involved in their configuration.\n",
        "\n",
        "**Flexibility and Customization**: High-level libraries abstract away many details, limiting your ability to modify core operations.\n",
        "With barebones code, you gain complete control over aspect, making it easier to tailor the model for specific use cases or integrate cutting-edge techniques.\n",
        "\n",
        "**Debugging and optimization**: By understanding each elemement of the pipeline, identifying and resolving bugs become more straightforward. Optimize process, minimizating data-loading overhead or adjusting augmentation strategies for performance gains.\n",
        "\n",
        "### **Constrasting Barebones Code With High-Level Libraries\n",
        "\n",
        "Detectron2 or MMDetection --> propduction-ready solutions --> Pre-configured architectures --> optimized training pipelines --> seamless integration --> pouplar frameworks.\n",
        "Their abstraction --> can obscure the underlying mechanics, leading to a \"black-box\" experience.\n",
        "\n",
        "For example, data loaders in high-level libraries often support complex data pipelines out-of-box --> This limits your ability to experiment with alternative loading strategies or debug edge cases.\n",
        "Similary, evaluation metrics are pre-configured --> can not suit custom tasks or specialized datasets.\n",
        "\n",
        "### The Educational Value of Rebuilding Components\n",
        "\n",
        "Building critical elements like -\n",
        "\n",
        "**Data Loaders**: Implementing custom DataLoader classes teaches you how data is batched, shuffled, and augmented.\n",
        "\n",
        "**Metrics**: Coding metrics such as mAP - mean Average Precision - provides insights into performance evaluation\n",
        "\n",
        "**Augmentations**: Manually applying augmentation --> model generalization\n",
        "\n",
        "Starting with barebones --> clear view of how CV models are structured and trained. --> Inovate, debug, and improve\n",
        "\n",
        "## Setting Up the Enviroment\n",
        "\n",
        "Building a lightweight custom OD model in PyTorch requires setting up the correct environment and organizing your project effectively.\n",
        "This ensures smooth development, scalability, and efficiency during the implementation of your model.\n",
        "\n",
        "### Key Dependencies\n",
        "\n",
        "```bash\n",
        "pip install torch\n",
        "pip istall torchvision\n",
        "CUDA - optional\n",
        "pip install matplotlib\n",
        "pip install matplotlib\n",
        "pip install numpy\n",
        "pip install pillow\n",
        "pip install scikit-learn tqdm\n",
        "```\n",
        "\n",
        "**Project Structure**: To keep things organized, structure your project with clear directories for code, dataset and logs\n",
        "\n",
        "```\n",
        "object_detection_project/\n",
        "├── data/\n",
        "│   └── train/\n",
        "│   └── val/\n",
        "├── models/\n",
        "│   └── yolov5.py\n",
        "│   └── faster_rcnn.py\n",
        "├── utils/\n",
        "│   └── transforms.py\n",
        "│   └── metrics.py\n",
        "├── scripts/\n",
        "│   └── train.py\n",
        "│   └── inference.py\n",
        "└── requirements.txt\n",
        "```\n",
        "\n",
        "This structure separatesd datasets, model definition, utilities, and scripts, making it easier to scale and manage.\n",
        "\n",
        "## Minimal Implementation of A Backbone\n",
        "\n",
        "### What is a Backbone in OD?\n",
        "\n",
        "The backbone of an OD model is a NN responsible for feature extraction.\n",
        "It process the input data (image) and generates feature maps that highlight important structures like edge, textures, and shapes.\n",
        "Common choices includes ResNet and MobileNet\n",
        "\n",
        "### Role of the Backbone\n",
        "\n",
        "**Feature Extraction**: Captures spatial hierarchies in the image, forming the basis for further processing by the detection head.\n",
        "\n",
        "**Transfer Learning**: Pre-trained backbones are often used to leverage existing knowledge, accelerating training on small datasets.\n",
        "\n",
        "**Efficiency**: Choice of backbone impacts the model's speed and accuracy.\n",
        "\n",
        "### Implementing A Single Convolutional Backbone in PyTorch\n",
        "\n",
        "We will use a minimal example to build and modify a ResNet-based backbone for OD.\n",
        "\n",
        "#### Step 1 - Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d4VxYwlIGHEA"
      },
      "outputs": [],
      "source": [
        "# Import Dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ca_n9cHDbtR",
        "outputId": "b70f4e36-91af-4c89-d356-46460cf42bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Setting up Accelerators\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct_9Qud1GFRo"
      },
      "source": [
        "### Step 2 - Define The Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iiDfJE5IHBxX"
      },
      "outputs": [],
      "source": [
        "class Backbone(nn.Module):\n",
        "  def __init__(self, pretrained=True, trainable_layers=3):\n",
        "    super(Backbone, self).__init__()\n",
        "\n",
        "    # Load ResNet with pre-trained weights\n",
        "    self.weights = models.ResNet50_Weights.DEFAULT # Best availiable\n",
        "    self.resnet= models.resnet50(weights=self.weights)\n",
        "\n",
        "    # Extract layers up to the final conv block of the ResNet50\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "                self.resnet.conv1,\n",
        "                self.resnet.bn1,\n",
        "                self.resnet.relu,\n",
        "                self.resnet.maxpool,\n",
        "                self.resnet.layer1,\n",
        "                self.resnet.layer2,\n",
        "                self.resnet.layer3,\n",
        "                self.resnet.layer4\n",
        "                )\n",
        "\n",
        "    # Optional freeze some layers for transfer learning - When use transfer Learning\n",
        "    layers_to_freeze = len(list(self.feature_extractor.children())) - trainable_layers\n",
        "    for i, layer in enumerate(self.feature_extractor.children()):\n",
        "      if i < layers_to_freeze:\n",
        "        for param in layer.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "  def forward(self,x): # Forward function\n",
        "    print(f\"x: {x.shape}\\n\")\n",
        "    x = self.feature_extractor(x)\n",
        "    print(f\"Backbone x.shape: {x.shape}\\n\")\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_1RfzWmPQkw"
      },
      "source": [
        "## Step 3 - Initialize the Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IvA0IrziPTND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a143d0-597f-46a1-9263-838a9433c0f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 151MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Build a backbone as instance\n",
        "backbone = Backbone(pretrained=True, trainable_layers=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLK5dXkkPqlo",
        "outputId": "bf0e6689-d5db-4ea8-f133-aa2b02ec15c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone(\n",
            "  (resnet): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            "  )\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(backbone)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhKlB_0WPwys"
      },
      "source": [
        "## Step 4 - Modifying Pre-Trained Backbones\n",
        "\n",
        "Sometimes,specific architectures require slight modifications to the backbone - adding extralayer or changing the output size. Here is an example of adding a custom output layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E-IzrPnU382X"
      },
      "outputs": [],
      "source": [
        "class CustomBackbone(Backbone):\n",
        "  def __init__(self, pretrained=True, trainable_layers=3, num_channels=512):\n",
        "    super(CustomBackbone, self).__init__(pretrained, trainable_layers)\n",
        "\n",
        "    # Add a 1x1 conv layer to reduce the output channels\n",
        "    self.conv1x1 = nn.Conv2d(2048, num_channels, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = super(CustomBackbone, self).forward(x)\n",
        "\n",
        "    x = self.conv1x1(x)\n",
        "    print(f\"CustomBackbone x.shape: {x.shape}\\n\")\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXX2A-q201H"
      },
      "source": [
        "## Testing the BackBone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCd2ms0P5LFJ",
        "outputId": "462a18a1-2503-4d75-cb7c-9361ba7c674b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dummy imput: torch.Size([2, 3, 224, 224])\n",
            "\n",
            "x: torch.Size([2, 3, 224, 224])\n",
            "\n",
            "Backbone x.shape: torch.Size([2, 2048, 7, 7])\n",
            "\n",
            "Output feature map size: torch.Size([2, 2048, 7, 7])\n",
            "\n",
            "Output fetaure map.shape: torch.Size([2, 2048, 7, 7])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example input - batch of 2 images, 3 channels, 224 x224\n",
        "dummy_imput = torch.randn(2,3,224,224)\n",
        "print(f\"dummy imput: {dummy_imput.shape}\\n\")\n",
        "\n",
        "# forward pass\n",
        "features = backbone(dummy_imput)\n",
        "\n",
        "print(f\"Output feature map size: {features.size()}\\n\")  # RETURN x!\n",
        "print(f\"Output fetaure map.shape: {features.shape}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDK5pfda8hVl"
      },
      "source": [
        "### **Key Takeways**\n",
        "\n",
        "**Pre-Trained Backbones Save Time**: Leveraging pre-trained weights accelerates convergence, especially with limited data.\n",
        "\n",
        "**Customization**: Adding or modifying layers tailors the backbone to specific use cases.\n",
        "\n",
        "**Freezing Layers**: Freezing early layers helps focus training on hig-level features.\n",
        "\n",
        "This minimal implementation provides a foundation to build custom OD piplines,--> enables experiment different architectures --> pretrained models.\n",
        "\n",
        "## Building A Barebone Detection Head\n",
        "### Purpose of the Detection Head\n",
        "\n",
        "The detection head in OD model process the features extracted by the backbone and generates:\n",
        "\n",
        "**Bouding Box Regression**: Predicts the coordinates of the bounding boxes around detected objects.\n",
        "\n",
        "**Class Prediction**: Assgins a class label - or \"background\" for no object - to each predicted bounding box.\n",
        "\n",
        "A detection head is typically a lightweight NN, often a combination of fully connectec (dense) layers, that processes spatial feature maps.\n",
        "\n",
        "### **Implementing A Simple Detection Head in PyTorch**\n",
        "\n",
        "Here --> implement a barebone --> multi-layer perceptron (MLP) --> detection head --> BB regression --> class prediction\n",
        "\n",
        "#### **Step 1 - Define The Detection Head**\n",
        "\n",
        "The detection head comprises two separate MLPs - one for **bounding box regression** and another for **class prediction**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gz5OjzDC_eew"
      },
      "outputs": [],
      "source": [
        "## A class for detection Head Example\n",
        "\n",
        "class DetectionHead(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "    super(DetectionHead, self).__init__()\n",
        "\n",
        "    # Bounding Box Regression Head\n",
        "    self.bbox_head = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1,1)),  # Use gloval average pooling to keep spatial infor - real detectors\n",
        "        nn.Flatten(), # Flattenize the input data - in this case the Image tensor.\n",
        "        nn.Linear(in_channels, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,4)  # output: [x_min, y_min, x_max, y_max]\n",
        "    )\n",
        "\n",
        "    # Class prediction head\n",
        "    self.class_head = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1,1)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_channels, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, num_classes)   # Output: logits for each class\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Bounding box regression\n",
        "    print(\"BEFORE bbox_predictions Detection Head\\n\")\n",
        "    bbox_predictions = self.bbox_head(x)\n",
        "    print(f\"bbox_predictions Detection Head: {bbox_predictions.shape}\\n\")\n",
        "\n",
        "    # Class predictions\n",
        "    print(\"BEFORE class_logits DETECTIONhead\\n\")\n",
        "    class_logits = self.class_head(x)\n",
        "    print(f\"class logits Detection Head: {class_logits}\\n\")\n",
        "\n",
        "    return bbox_predictions, class_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FvAr98OEZYu"
      },
      "source": [
        "#### **Step 2 - Integrate the Head With the Backbone**\n",
        "\n",
        "We now combine the **Backbone** and **DetectionHead** to create a complete object detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G-hUorpYE9kr"
      },
      "outputs": [],
      "source": [
        "class BarebonesObjectDetector(nn.Module):\n",
        "  def __init__(self, backbone, num_classes):\n",
        "    super(BarebonesObjectDetector, self).__init__()\n",
        "    self.backbone = backbone\n",
        "    self.detection_head = DetectionHead(in_channels=2048, num_classes=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # extract features using the backbone\n",
        "    features = self.backbone(x)\n",
        "    print(f\"BarebonesObjectDetector features: {features.shape}\")\n",
        "\n",
        "    # forward pass trough the detection hed\n",
        "    bbox_predictions, class_logits = self.detection_head(features)\n",
        "    print(f\"BarebonesObjectDetector bbox_pred: {bbox_predictions.shape}\")\n",
        "    print(f\"BarebonesObjectDetector class_logits: {class_logits.shape}\")\n",
        "\n",
        "    return bbox_predictions, class_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OMaypq7HL7C"
      },
      "source": [
        "#### **Step 3 - Initialize the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-xGdMTsHSF6",
        "outputId": "0e686615-1039-4791-e5e0-4ccd6216a771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BarebonesObjectDetector(\n",
            "  (backbone): Backbone(\n",
            "    (resnet): ResNet(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            "    )\n",
            "    (feature_extractor): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (5): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (6): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (7): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (detection_head): DetectionHead(\n",
            "    (bbox_head): Sequential(\n",
            "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "      (1): Flatten(start_dim=1, end_dim=-1)\n",
            "      (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      (3): ReLU()\n",
            "      (4): Linear(in_features=512, out_features=4, bias=True)\n",
            "    )\n",
            "    (class_head): Sequential(\n",
            "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "      (1): Flatten(start_dim=1, end_dim=-1)\n",
            "      (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      (3): ReLU()\n",
            "      (4): Linear(in_features=512, out_features=11, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# define the number of classes - 10 objects + 1 background class\n",
        "num_classes = 11\n",
        "\n",
        "# use the backbone implemented earlieer\n",
        "backbone = Backbone(pretrained=True, trainable_layers=3)\n",
        "\n",
        "# Build the full object detection model\n",
        "model = BarebonesObjectDetector(backbone=backbone, num_classes=num_classes)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVUWVsIyogBy"
      },
      "source": [
        "### Step 4 -Testing the Model\n",
        "\n",
        "We will pass a dummy batch of images through the model to verify its functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjZLYqWnorlU",
        "outputId": "45c19628-5a8a-46ca-8490-b81d8aba820c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: torch.Size([2, 3, 224, 224])\n",
            "\n",
            "Backbone x.shape: torch.Size([2, 2048, 7, 7])\n",
            "\n",
            "BarebonesObjectDetector features: torch.Size([2, 2048, 7, 7])\n",
            "BEFORE bbox_predictions Detection Head\n",
            "\n",
            "bbox_predictions Detection Head: torch.Size([2, 4])\n",
            "\n",
            "BEFORE class_logits DETECTIONhead\n",
            "\n",
            "class logits Detection Head: tensor([[-0.0100, -0.0340,  0.0309, -0.0218,  0.0057, -0.0339,  0.0176, -0.0401,\n",
            "          0.0726, -0.0189,  0.0658],\n",
            "        [ 0.0125, -0.0264, -0.0091, -0.0122,  0.0281, -0.0176, -0.0293, -0.0477,\n",
            "          0.0604, -0.0393, -0.0045]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "BarebonesObjectDetector bbox_pred: torch.Size([2, 4])\n",
            "BarebonesObjectDetector class_logits: torch.Size([2, 11])\n",
            "Bounding Box predictions: torch.Size([2, 4])\n",
            "\n",
            "Class Predictions: torch.Size([2, 11])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example input - batch of 2 images, 3 channels, 224 x 224\n",
        "dummy_input = torch.randn(2,3,224,224)\n",
        "\n",
        "# Forward pass\n",
        "bbox_predictions, class_logits = model(dummy_imput)\n",
        "\n",
        "# output sizes\n",
        "print(f\"Bounding Box predictions: {bbox_predictions.size()}\\n\")  # Should be [batch_size, 4]\n",
        "print(f\"Class Predictions: {class_logits.size()}\\n\")  # Should be [batch_size, num_classes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-2Whff3K7j5"
      },
      "source": [
        "#### **Key feature of This Implementation**\n",
        "\n",
        "**Separation of Concerns**: The detection head is modular, allowing easy replacement or enhancement.\n",
        "\n",
        "**Scalability**: Architecture --> Support multiple classes and custom backbone integrations\n",
        "\n",
        "**Simplicity**: Using MLPs for bounding box regression and classification provides a clear foundation for more complex heads like those in Faster R-CNN or YOLO.\n",
        "\n",
        "This basic detection head is a stepping stone to understanding and implementing custom OD pipelines.\n",
        "\n",
        "## **Implementing Forward Passes & Loss Functions**\n",
        "### **Overview**\n",
        "\n",
        "A forward pass in OD model involves -\n",
        "\n",
        "Passing input image --> backbone --> features\n",
        "\n",
        "Feeding features into detection head --> Bounding boxes --> class logits\n",
        "\n",
        "Calculating losses for BB regression and classification.\n",
        "\n",
        "Implement --> step-by-step\n",
        "\n",
        "### **Loss function**\n",
        "#### **BB Regression Loss**\n",
        "\n",
        "We use **Smoth L1 Loss** (Huber loss) --> balance sensitivity to outliers while penalizing larger errors more significantly\n",
        "\n",
        "#### **Classification Loss**\n",
        "\n",
        "We use **Cross-Entropy Loss** for class logits. It measures the difference between predicted class probabilities and true labels.\n",
        "\n",
        "### **Implementing the Loss Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiUW5SndPJ-t"
      },
      "source": [
        "### **Forward Pass - From input to Predictions**\n",
        "\n",
        "Forward pass --> uses combined backbone and detection head implemented earlier.\n",
        "For training --> ground truth annotations --> BB and class labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDazwSxwE95k"
      },
      "outputs": [],
      "source": [
        "# A function to calculate the loss\n",
        "def compute_losses(bbox_predictions, class_logits, targets):\n",
        "  \"\"\"\n",
        "    Compute the total loss for BB and classification\n",
        "\n",
        "    Args:\n",
        "      bbox_predictions (torch.Tensor): Predicted BB, shape [btach_size, num_classes].\n",
        "      targets (ditct): Ground truth with keys:\n",
        "        -'boxes': Ground truth BB, shape [batch_size, 4]\n",
        "        - 'labels': Ground truth class labels, shape [batch_size]\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: Total loss (sum of regression and classficaton losses)\n",
        "  \"\"\"\n",
        "\n",
        "  # Smooth L1 Loss for BB regression\n",
        "  bbox_loss_fn = nn.SmoothL1Loss()\n",
        "  bbox_loss = bbox_loss_fn(bbox_predictions, targets['boxes'])\n",
        "\n",
        "  # Cross Entropy Loss for classification\n",
        "  class_loss_fn = nn.CrossEntropyLoss()\n",
        "  class_loss = class_loss_fn(class_logits, targets['labels'])\n",
        "\n",
        "  # Total loss\n",
        "  total_loss = bbox_loss + class_loss\n",
        "\n",
        "  return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSde65z1NS40"
      },
      "outputs": [],
      "source": [
        "# A forward pass function for OD\n",
        "\n",
        "def forward_pass(model, images, targets=None):\n",
        "  \"\"\"\n",
        "  Performa a forward pass through the model.\n",
        "\n",
        "  Args:\n",
        "    model (nn.Module): the OD model\n",
        "    images (torch.Tensor): input image of shape [batch_size, channels, heigh, width]\n",
        "    targets (dict): Ground truth with keys 'boxes' and 'labels'.\n",
        "\n",
        "  Returns:\n",
        "    Tuple: Predicted BB and class logits - during inference,\n",
        "    or total loss - during training.\n",
        "  \"\"\"\n",
        "\n",
        "  # perform forward pass trough the model\n",
        "  bbox_predictions, class_logits = model(images)  # this is the forward pass into the model\n",
        "  if targets:\n",
        "    # if training, compute losses\n",
        "    loss = compute_losses(bbox_predictions, class_logits, targets)\n",
        "\n",
        "    return loss\n",
        "  else:\n",
        "    # if inference, return predictions\n",
        "    return bbox_predictions, class_logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsEaNAChF5W1"
      },
      "source": [
        "### **Training Example - Calculating Total Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjwbTE6dF-ir"
      },
      "outputs": [],
      "source": [
        "# A training example with dummy data.\n",
        "num_classes = 11  # 10 objects + 1 background\n",
        "model = BarebonesObjectDetector(backbone, num_classes)\n",
        "\n",
        "# Example input images (batch_size, channels, height, width)\n",
        "images = torch.randn(2,3,224,224)\n",
        "\n",
        "# ground truth - dummy data\n",
        "targets = {\n",
        "    'boxes': torch.tensor([[50,60,150,200],[30,40,120,160]], dtype=torch.float32), # boxes shape: [btach_size,4]\n",
        "    'labels': torch.tensor([3,5], dtype=torch.long)  # labels shape: {batch_size}\n",
        "}\n",
        "\n",
        "# Forward pass and compute loss\n",
        "loss = forward_pass(model, images, targets)\n",
        "print(f\"Total Loss: {loss.item()}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INXNsIa8K0ad"
      },
      "source": [
        "#### **Explanation of The Process**\n",
        "\n",
        "**Input Images**: A batch of images is fed into the model.\n",
        "\n",
        "**Feature Extraction**: The backbone extracts feature maps from the images.\n",
        "\n",
        "**Bounding Box & Class Prediction**: The detection head predicts bounding boxes and class logits.\n",
        "\n",
        "**Loss Computation**: BB regression loss is computed between predicted BB and ground truth boxes. Classification loss compares predicted class probabilities with ground truth labels.\n",
        "\n",
        "**Total Loss**: Easy to replace or fine-tune specific components - loss function, detection head.\n",
        "\n",
        "**Customization**: Ground truth processing, such as custom label encoding, can be easily integrated.\n",
        "\n",
        "**Simplicity**: Provides a clear understanding of how BB regression and classification are optimized.\n",
        "\n",
        "## **Customizer Anchor Generation and NMS**\n",
        "### Introduction\n",
        "\n",
        "In OD, **anchor generation** and **non-maximum suppression (NMS)** are essential components for predicting and refining BB,\n",
        "\n",
        "**Anchor generation**: Builds predefined BB -anchors-- of various sizes and aspect ratios at different positions in the image.\n",
        "\n",
        "**Non-Maximum Suppression -NMS**: Removes overlapping boxes and retains the most confident prediction for each object. Customizing these steps allows developers to optimize model performance for specific datasets and use cases.\n",
        "\n",
        "###**Anchor Generation - The Foundation of Detection**\n",
        "**What is Anchor Generation?**\n",
        "\n",
        "Anchors are gris-based BB that help the model predict object locations.\n",
        "During training, each anchor is assigned a label based on its IoU with ground truth boxes.\n",
        "\n",
        "**Code for Anchor Generation**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3P1eoxoNqFd"
      },
      "outputs": [],
      "source": [
        "def generate_anchors(base_size=16, scales=[0.5,1.0,2.0], aspect_ratios=[0.5,1.0,2.0]):\n",
        "  \"\"\"\n",
        "    Generate anchor boxes based on scales and aspect ratios.\n",
        "\n",
        "    Args:\n",
        "      base_size (int): The size of the base anchor.\n",
        "      scales (list): Scaling factors for anchors.\n",
        "      aspect_ratios (list): Aspect ratios for anchors.\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: Generated anchors of shape [num_anchors, 4] (x_min, y_min, x_max, y_max)\n",
        "  \"\"\"\n",
        "\n",
        "  anchors = []\n",
        "  for scale in scales:\n",
        "    for ratio in aspect_ratios:\n",
        "      w = base_size * scale * (ratio**0.5)\n",
        "      h = base_size / scale / (ratio**0.5)\n",
        "      x_min, y_min = -w / 2, -h / 2\n",
        "      x_max, y_max = w / 2, h / 2\n",
        "      anchors.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "  return torch.tensor(anchors, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw1NF4_0Plu9"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "anchors = generate_anchors()\n",
        "print(anchors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46LiTc3lZn3O"
      },
      "source": [
        "### **Key Considerations**:\n",
        "\n",
        "**Base Size**: Determines the scale of the grid.\n",
        "\n",
        "**Scales & Aspect Ratios**: Should match the size and shapes of objects in the dataset.\n",
        "\n",
        "#### **Non-Maximum Suppression -- Filtering Overlaps**\n",
        "\n",
        "**What is NMS**\n",
        "\n",
        "NMS ensures only the most confident bounding box is retained for Overlapping detections.\n",
        "It uses a confidence score threshold and and IoU threshold to filer predictions.\n",
        "\n",
        "**Code For NMS in PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhBvse8r6-O7"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(box1, box2):\n",
        "  \"\"\"\n",
        "    Compute IoU between two sets of boxes.\n",
        "\n",
        "    Args:\n",
        "      box1 (torch.Tensor): Single box, shape [1,4]\n",
        "      box2 (torch.Tensor): Multiple boxes, shape [N,4]\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: IoU scores for each box in box2.\n",
        "  \"\"\"\n",
        "\n",
        "  inter = (\n",
        "      torch.min(box1[:,2], box2[:,2]) - torch.max(box1[:,0], box2[:,0])\n",
        "  ).clamp(0) * (\n",
        "      torch.min(box1[:,3], box2[:,3]) - torch.max(box1[:,1],box2[:,1])\n",
        "  ).clamp(0)  # .clamp --> Clamps all elements in input into the range [ min, max ]. Letting min_value and max_value be min and max, respectively, this returns:\n",
        "              #yi=min⁡(max⁡(xi,min_valuei),max_valuei)y i​ =min(max(x i​ ,min_value i​ ),max_value i​)\n",
        "\n",
        "  box1_area = (box1[:, 2] - box1[:,0]) * (box1[:, 3]- box1[:,1])\n",
        "  box2_area = (box2[:, 2] - box2[:,0]) * (box2[:, 3]- box2[:,1])\n",
        "\n",
        "  union = box1_area + box2_area - inter\n",
        "  IoU = inter / union\n",
        "\n",
        "  return IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjKCWMRkakW9"
      },
      "outputs": [],
      "source": [
        "def non_maximum_suppresion(boxes, scores, iou_threshold=0.5):\n",
        "  \"\"\"\n",
        "    Perform non-maximum suppresion - NMS - on BB\n",
        "\n",
        "    Args:\n",
        "      boxes (torch.Tensor): Predicted boxes, shape [num_boxes, 4]\n",
        "      scores (torch.Tensor): Confidence_scores, shape [num_boxes].\n",
        "      ior_threshold (float): IoU threshold for NMS\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: Indides of the retained boxes.\n",
        "  \"\"\"\n",
        "\n",
        "  indices = torch.argsort(scores, descending=True)\n",
        "  keep = []\n",
        "\n",
        "  while indices.numel() > 0:\n",
        "    current = indices[0]\n",
        "    keep.append(current)\n",
        "\n",
        "    if indices.numel() == 1:\n",
        "      break\n",
        "\n",
        "    remaining_boxes = boxes[indices[1:]]\n",
        "    iou = calculate_iou(boxes[current].unsqueeze(0), remaining_boxes)\n",
        "    indices = indices[1:][iou < iou_threshold]\n",
        "\n",
        "  return torch.tensor(keep, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okiosAnE3tfI"
      },
      "source": [
        "#### **Impact of Anchor Sizes & IoU Threshold**\n",
        "\n",
        "**Anchor Sizes**: Larger anchorrs suit larger objects, while smaller anchors better capture small objects.\n",
        "\n",
        "**IoU Threshold**: A **lower IoU threshold** may retain more overlapping boxes - increasinf Recall but reducing precision.\n",
        "A **higher IoU threshold** favors precision by keeping only the most confident boxes.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "* Small Objects: smaller base sizes and lower IoU thresholds.\n",
        "* Large Objects: Larger base sizes and higher IoU thresholds.\n",
        "\n",
        "Customizing anchor classification and NMS --> fine tunning OD --> specific datastes.\n",
        "\n",
        "## **Training the Model with Custom Pipelines**\n",
        "\n",
        "Training OD --> involves integratiing data preparation, model architecture, loss function, and optimization --> cohesive overflow.\n",
        "With barenone PyTorch --> full control over these compoenents.\n",
        "\n",
        "###**Custom DataLoaders & Preprocessing Pipelines**\n",
        "####**Data Preparation**\n",
        "\n",
        "In barebones implementation, building custom Dataset and DataLoader classes is essential for loading and preprocessing data.\n",
        "\n",
        "**Code Example - Custom Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8SThLM0-r4C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomObjectDetectionDataset(Dataset): # use Dataset from PyTorch\n",
        "  def __init__(self, annotation, image_dir, transform=None):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        annotations (list): list of dictionaries containing images paths and labels.\n",
        "        image_dir (str): Directory with all the images\n",
        "        transform (callable, optional): Transformations to be applied on an sample\n",
        "    \"\"\"\n",
        "    self.annotation = annotation\n",
        "    self.image_dir = image_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotation)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    annotation = self.annotation[idx]\n",
        "    image_path = f\"{self.image_dir}/{annotation['image_name']}\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    boxes = torch.tensor(annotation['boxes'], dtype=torch.float32)\n",
        "    labels = torch.tensor(annotation['label'], dtype=torch.long)\n",
        "\n",
        "    sample = {\"image\": image, \"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "    if self.transform:\n",
        "      sample[\"image\"] = self.transform(sample[\"image\"])\n",
        "\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35Elqp0zpMQh"
      },
      "outputs": [],
      "source": [
        "# downloading the COCO dataset ANOTATIONS for example usage\n",
        "# Getting files\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "# setup path to a datafolder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"images\"\n",
        "\n",
        "\n",
        "# if the image folder does not exist, downlowd it and prepara it\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory alredy exist... skipping download\")\n",
        "else:\n",
        "  print(\"Image path does not exist, building it\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "url = f\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n",
        "#f\"http://images.cocodataset.org/annotations/image_info_test2017.zip\"\n",
        "local_file = data_path / f\"coco_annotations.zip\"\n",
        "print(f\"Downloading {url}\")\n",
        "request = requests.get(url)\n",
        "\n",
        "with open(local_file, \"wb\") as f:\n",
        "  f.write(request.content)\n",
        "  print(f\"Saved {local_file}\")\n",
        "\n",
        "\n",
        "# Now extract the final zip\n",
        "zip_path = data_path / \"coco_annotations.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "  print(f\"Extracting {zip_path}\")\n",
        "  zip_ref.extractall(image_path)\n",
        "  print(f\"{zip_path} extracted into {image_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4PPZLJFl87z"
      },
      "outputs": [],
      "source": [
        "!wget -O data/coco_images.zip http://images.cocodataset.org/zips/train2014.zip\n",
        "#http://images.cocodataset.org/zips/test2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfE3-kMgmDVm"
      },
      "outputs": [],
      "source": [
        "# This part of the code can ben changed in some final version - it is only to get the dataset to tests of the algorithms\n",
        "!unzip -q data/coco_images.zip -d data/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqlfISgbsv_u"
      },
      "outputs": [],
      "source": [
        "# # downloading the COCO DATASET for example usage\n",
        "# url = f\"http://images.cocodataset.org/zips/test2017.zip\"\n",
        "# local_file = data_path / f\"coco_images.zip\"\n",
        "# print(f\"Downloading {url} to {local_file}\")\n",
        "\n",
        "# #request = requests.get(url)\n",
        "\n",
        "# with requests.get(url, stream=True) as r:\n",
        "#   r.raise_for_status()\n",
        "\n",
        "#   with open(local_file, \"wb\") as f:\n",
        "#     for chunk in r.iter_content(chunk_size=8182):\n",
        "#       if chunk:\n",
        "#         f.write\n",
        "\n",
        "# # with open(local_file, \"wb\") as f:\n",
        "# #   f.write(request.content)\n",
        "# #   print(f\"Saved {local_file}\")\n",
        "# print(f\"✅ Saved{local_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOVxQKIJf0Cz"
      },
      "outputs": [],
      "source": [
        "# # Now extract the final zip\n",
        "# import os\n",
        "# #zip_path = image_path / \"coco_images.zip\"\n",
        "# print(f\"zip_file: {local_file }\")\n",
        "# print(f\"File size: {os.path.getsize(local_file) / (1024*1024):.2f}MB\")\n",
        "# with zipfile.ZipFile(local_file , \"r\") as zip_ref:\n",
        "#   print(f\"Extracting {local_file }\")\n",
        "#   zip_ref.extractall(image_path )\n",
        "#   print(f\"{local_file} extracted into {image_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMrhJ0fktXqF"
      },
      "outputs": [],
      "source": [
        "# Now getting the annotations variable\n",
        "with open(\"./data/images/annotations/captions_train2014.json\", \"r\") as g:\n",
        "  annotations = json.load(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwtbrI94V-Lm"
      },
      "outputs": [],
      "source": [
        "# Desting the code of CustomObject\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((300,300)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "dataset = CustomObjectDetectionDataset(annotations,\"./data/images/train2014/\", transform) # should define annotations -- see in next chapters\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"type(dataset.annotation): {type(dataset.annotation)}\")\n",
        "print(\"len(dataset.annotation): \", len(dataset.annotation) if hasattr(dataset.annotation, '__len__') else \"no length\")\n",
        "#print(dataset.annotation) # NÃO DESCOMENTAR!"
      ],
      "metadata": {
        "id": "NlXYFKJ5FD4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We5RGX-8BX_Z"
      },
      "source": [
        "### **Training Loop with Barebones PyTorch Code**\n",
        "\n",
        "#### **Steps in the Training Loop**:\n",
        "\n",
        "1. Load data batches using DataLoader.\n",
        "2. Perform a forward pass through the model.\n",
        "3. Compute losses\n",
        "4. Backpropagate and update model parameters\n",
        "\n",
        "### **Code Example - Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlJrKvHPJR1Z"
      },
      "outputs": [],
      "source": [
        "# a code example for training loop for detection\n",
        "import torch.optim as optim\n",
        "def train_model_detection(model: nn.Module, dataloader: torch.utils.data.DataLoader, num_epochs: int, learning_rate: float):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # can be seted manually or by args; # Define the optimizer\n",
        "  for epoch in range(num_epochs): # define the total epochs - can be adapted by training for batches and to insert this code in Daniel Bourke Example for training, validation and test loops\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader: # Here get the batches\n",
        "      images = batch['image'].to(device)  # get the entire image\n",
        "      boxes = batch['boxes'].to(device)  # get the bbox\n",
        "      labels = batch['labels'].to(device)  # get the labels\n",
        "\n",
        "      # reset gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward_pass   # bbox_predictions, class_logits, targets\n",
        "      outputs = model(images)\n",
        "      loss = compute_losses(boxes, outputs, labels)\n",
        "\n",
        "      # backward pass and optimization\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cc-_CZaS_a-"
      },
      "outputs": [],
      "source": [
        "# num_epochs = 2\n",
        "# learning_rate = 0.001\n",
        "# train_model_detection(model, dataloader, num_epochs, learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRXcShs_jl-7"
      },
      "source": [
        "### **Debugging Techiniques for Model Convergence**\n",
        "\n",
        "1. **Validade Data and labels**:\n",
        "Ensure images, bounding boxes, and labels are loaded correctly\n",
        "\n",
        "Visualize samples using matplotlib to confirm proper data augmentation and label alignment.\n",
        "\n",
        "2. **Monitor Loss Trends**:\n",
        "Check if the total loss decreases over epochs. If not:\n",
        "\n",
        "Very gradients are updating - torch.autograd hooks.\n",
        "\n",
        "Smaller learning rate\n",
        "\n",
        "3. **Overfit On a Small Batch**:\n",
        "Test the model ability to fit on a single batch.\n",
        "Raid convergence indicates a functional architecture.\n",
        "\n",
        "4. **Inspect Outputs**:\n",
        "Compare predictions - BB and labels -- ground truth\n",
        "Use debugging prints.\n",
        "\n",
        "5. **Adjust Anchor & Hyperparameters**:\n",
        "Poor detection accuracy may indicate a mismatch between anchor sizes and dataset object dimension;\n",
        "\n",
        "Training an OD model --> custom pipelne --> maximum flexibility.\n",
        "\n",
        "## **Evaluating Object Detection Models**\n",
        "\n",
        "Evaluation is critical step in OD workflows --> ensuring your model performs well on unseen data.\n",
        "Key metrics --> mAP, IoU, Precision, R, F-Score\n",
        "\n",
        "#### **Key metrics for OD**\n",
        "* **IoU**: Measures overlap between predicted BB and ground truth boxes. IoU thresholds - 0.5, 0.75 - define wheter a detection is a TP.\n",
        "* **mAP**: Primary metric for evaluating OD models.\n",
        "  * Steps:\n",
        "    * Compute Precision + Recall --> different confidence thresholds.\n",
        "    * Calculate AP for each class.\n",
        "    * Take mAP across all classes\n",
        "    * Evaluated at specific IoU thresholds - mAP50, mAP95\n",
        "* **Precision-Recall Curve**: CONTINUE FROM HERE\n",
        "\n",
        "**Precision**: Fraction of true positives among all predicted positives.\n",
        "**Recall**: Fraction of true positives among all actual positives\n",
        "\n",
        "Plotting PR curve helps visualize trade-offs.\n",
        "\n",
        "### **Minimalist Evaluation Script**\n",
        "\n",
        "#### **Code Example - Evaluation mAP & IoU**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwlevQgnkdtn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "  \"\"\"\n",
        "    Computes IoU between two boxes\n",
        "  \"\"\"\n",
        "  x1 = max(box1[0], box2[0])\n",
        "  y1 = max(box1[1], box2[1])\n",
        "  x2 = min(box1[2], box2[2])\n",
        "  y2 = min(box1[3], box2[3])\n",
        "\n",
        "  inter_area = max(0, x2 - x1) * max(0, y2 - y1)  # get the intersection area\n",
        "  box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1]) # get the area of box1\n",
        "  box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1]) # get the area of box2\n",
        "  union_area = box1_area + box2_area - inter_area # get the union of the area of the 2 boxes\n",
        "\n",
        "  return inter_area / union_area if union_area > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, iou_threshold=0.5):\n",
        "  \"\"\"\n",
        "  Evaluate mAP for the object detection model\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  all_precisions = []\n",
        "  all_recalls = []\n",
        "\n",
        "  for batch in dataloader:\n",
        "    images = batch[\"image\"]  # get the image\n",
        "    true_boxes = batch['boxes'] # the the true boxes\n",
        "    true_labels = batch['labels']  # get the true labels\n",
        "\n",
        "    with torch.no_grad():\n",
        "      predictions = model(images)\n",
        "\n",
        "    for i, pred in enumerate(predictions):\n",
        "      pred_boxes = pred['boxes']\n",
        "      pred_scores = pred['scores']\n",
        "      gt_boxes = true_boxes[i]\n",
        "\n",
        "      # calculate IoU for each prediction\n",
        "      matches = []\n",
        "      for pred_box in pred_boxes:\n",
        "        ious = [compute_iou(pred_box, gt_box) for gt_box in gt_boxes]\n",
        "        matches.append(max(ious) >= iou_threshold)\n",
        "\n",
        "      tp = sum(matches)\n",
        "      fp = len(matches) - tp\n",
        "      fn = len(gt_boxes) - tp\n",
        "\n",
        "      precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "      recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "\n",
        "      all_precisions.append(precision)\n",
        "      all_recalls.append(recall)\n",
        "\n",
        "  mAP = np.mean(all_precisions)\n",
        "  print(f\"mAP: {mAP:.4f}\")\n"
      ],
      "metadata": {
        "id": "m0MF2GoFPS7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate_model(model, test_dataloader)"
      ],
      "metadata": {
        "id": "Ox3VeFJTREsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling Up - Modularity & Optimization\n",
        "Scaling and OD --> model from barebones implementation to a production-grade system --> focus on modularity and optmization.\n",
        "Modular code enhances reusability and experimentation, while optimization techniques improve performance and reduce training time.\n",
        "\n",
        "### **Modularizing your code**\n",
        "Essential --> different components --> backbones --> detection heads --> loss functions\n",
        "\n",
        "#### **Component-Based Architecture**\n",
        "\n",
        "Divide the model into independent modules\n",
        "\n",
        "* **Backbone**: Feature extraction --> Resnet - MobilNet.\n",
        "* **Head**: Classification and Regression layers.\n",
        "* **Anchor Generator**: Generate anchor boxes.\n",
        "* **Loss Functions**: Compute classification and bounding box losses.\n",
        "\n",
        "**Examples of Modularization**:\n",
        "```python\n",
        "class CustomObjectDetector(torch.nn.Module):\n",
        "  def __init__(self, backbone, detection_head, anchor_generator):\n",
        "    super(CustomObjectDetector, self).__init__()\n",
        "    self.backbone = backbone  # the network backbone\n",
        "    self.detection_head = detection_head  # the NN used as detector\n",
        "    self.anchor_generator = anchor_generator # the code that generate anchors\n",
        "\n",
        "  def forward(self, images):\n",
        "    features = self.backbone(images)  # get the features from backbone NN\n",
        "    anchors = self.anchor_generator(features)\n",
        "    predictions = self.detection_head(features, anchors)\n",
        "\n",
        "    return predictions\n",
        "```\n",
        "\n",
        "### **Configuration-Driven Design**\n",
        "\n",
        "Use configuration files - YAML or JSON - to define models parameters, datasets, and training operations.\n",
        "\n",
        "This approach simplifies switching between experiments.\n",
        "\n",
        "### **Optimzation Techiniques**:\n",
        "####**Mixed Precision Training**\n",
        "Leverage lower precision - float16 - for faster computations while maintatining acc.\n",
        "\n",
        "**Code Snippet for Mixed Precision**\n",
        "```python\n",
        "  # Test if it will work\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  for images, targets in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    with torch.cuda.amp.autocast():\n",
        "      predictions = model(images)\n",
        "      loss = compute_loss(predictions, targets)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "```\n",
        "\n",
        "#### ** Distributed Data Parallelism**\n",
        " Use multiple GPU to parallelize training.\n",
        "\n",
        " ```python\n",
        "  model = torch.nn.parallel.DistributedDataParallel(model)\n",
        " ```\n",
        "\n",
        "#### **Profiling & Bottleneck Detection**\n",
        "\n",
        "Use tools like torch.profiler to identify performance bottlenecks.\n",
        "\n",
        "Optimize data loading, matrix operations, and reduntante computations.\n",
        "\n",
        "```python\n",
        "with torch.profiler.profile(\n",
        "  schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
        "  on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),\n",
        "  record_shapes=True,\n",
        "  with_stack=True\n",
        ") as prof:\n",
        "    for images, targets in dataloader:\n",
        "        predictions = model(images)\n",
        "prof.export_chrome_trace(\"trace.json\")\n",
        "  \n",
        "```\n",
        "\n",
        "## **Real-World Use Case - Adapting to Custom Datasets\n",
        "\n",
        "Adapting an OD model to a custom dataset involves several key steps - dataset preparation, annotation formatting, and custom data loading. By understanding these process, you can seamlessly integrate your barebones OD model into specialized workflows.\n",
        "\n",
        "### **Dataset Preparation**\n",
        "\n",
        "#### **Define the Dataset**:\n",
        "Identify the specific classes and tasks for detection --> animals, vehicles, objects... -\n",
        "Training --> Validation --> Test --> datasets - cross validation\n",
        "\n",
        "#### **Anotation**\n",
        "Annotations --> BB --> class labels --> Segmenation masks --> optional\n",
        "\n",
        "* Popular formats:\n",
        "  * **COCO** - JSON based --> support BB, segmentation masks, keypoints.\n",
        "  * **Pascal VOC**: XML-based format for BB and labels\n",
        "  * **YOLO**: text files with class and BB coordinates normalized to image size.\n",
        "\n",
        "  If your annotations are no in standard format --> may convert them\n",
        "\n",
        "  ### **Converting Annotations to COCO Format**\n",
        "\n",
        "  If your annotations are not in a standard format, you may need to convert them.\n",
        "\n",
        "  **Example - Converting A CSV to COCO format**:\n",
        "  "
      ],
      "metadata": {
        "id": "vjdvM4CfSIaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def csv_to_coco(csv_file, images_dir, output_file):\n",
        "  coco_format = {\n",
        "      \"images\": [],\n",
        "      \"annotations\": [],\n",
        "      \"categories\": []\n",
        "  }\n",
        "\n",
        "  categories = {\"person\": 1, \"car\": 2, \"dog\": 3}   # Examples categories\n",
        "  annotations\n",
        "  for idx, (filename, class_name, xmin, ymin, xmax, ymax) in enumerate(csv_file):\n",
        "    coco_format[\"images\"].append({\n",
        "        \"idx\": idx,\n",
        "        \"filne_name\": filename,\n",
        "        \"width\": 1280,  # exmaple width\n",
        "        \"height\": 720, # example height\n",
        "    })\n",
        "\n",
        "    coco_format[\"annotations\"].append({\n",
        "        \"id\": annotation_id,\n",
        "        \"image_id\": idx,\n",
        "        \"category_id\": categories[class_name],\n",
        "        \"bbox\": [xmin, ymin, xmax - xmin, ymax - ymin],\n",
        "        \"area\": (xmax - xmin) * (ymax - ymin),\n",
        "        \"iscrowd\": 0\n",
        "    })\n",
        "    annotation_id += 1\n",
        "\n",
        "  for name, id in categories.items():\n",
        "    coco_format[\"categories\"].append({\"id\": id, \"name\": name})\n",
        "\n",
        "  with open(output_file, 'w') as f:\n",
        "    json.dump(coco_format, f, indent=4)"
      ],
      "metadata": {
        "id": "pDruZQhNFr6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usage # uncomment below\n",
        "#csv_to_coco(csv_file=\"annotations.csv\", images_dir=\"images/\", output_file=\"dataset.json\")\n"
      ],
      "metadata": {
        "id": "DW_6V0_xGA3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading Custom Data in PyTorch**\n",
        "\n",
        "**Custom Datasets Class**: torch.utils.data.Dataset --> to build a dataset class tailored to your annotations and preprocessing needs.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "K5n3PmsGMLYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, annotations, images_dir, transforms=None):\n",
        "    # load the JSON file\n",
        "    with open(annotations, \"r\") as f:\n",
        "      self.annotations = json.load(f)\n",
        "      #annotations\n",
        "    self.images_dir = images_dir\n",
        "    self.transforms = transforms\n",
        "\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations[\"images\"])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_info = self.annotations[\"images\"][idx]\n",
        "    #img_id = self.annotations[\"images\"][idx][\"id\"]\n",
        "    img_id = img_info[\"id\"]\n",
        "    image_path = os.path.join(self.images_dir, img_info[\"file_name\"])\n",
        "                              #self.annoatations[\"images\"][idx][\"file_name\"])\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # get all annotations for this image\n",
        "    # anns = [a for a in self.coco[\"annotations\"] if a[\"image_id\"] == image_id]\n",
        "\n",
        "    # boxes = [ann[\"bbox\"] for ann in anns]\n",
        "    # labels = [ann[\"category_id\"] for ann in anns]\n",
        "\n",
        "    # if self.transform:\n",
        "    #     image = self.transform(image)\n",
        "\n",
        "    # target = {\n",
        "    #     \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "    #     \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
        "    # }\n",
        "    #Extract BB and labels\n",
        "\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for ann in self.annotations[\"annotations\"]:\n",
        "      if ann[\"image_id\"] == img_id:\n",
        "        boxes.append(ann[\"bbox\"])\n",
        "        labels.append(ann[\"category_id\"])\n",
        "\n",
        "    # convert to tensors\n",
        "    boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "    labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "    target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "    if self.transforms:\n",
        "      image, target = self.transforms(image, target)\n",
        "\n",
        "    return image, target"
      ],
      "metadata": {
        "id": "TY7_rgiDMkGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In DL images usually have different sizes - width/height -> if tehy are tensors --> PyTorch can't stack into one batch without resizing or padding\n",
        "# Custom collate_fn --> avoid the error: TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
        "# collation function (default_collate) is trying to stack items in a batch, but it cannot stack PIL images directly\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "RuezWASIFBfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(annotations=\"./data/images/annotations/instances_train2014.json\", images_dir=\"./data/images/train2014/\")"
      ],
      "metadata": {
        "id": "K4fQ7OkJR8Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                         batch_size=4,\n",
        "                                         shuffle=True,\n",
        "                                         collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "D0RR_llKTUpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "learning_rate = 0.001\n",
        "train_model_detection(model, dataloader, num_epochs, learning_rate)"
      ],
      "metadata": {
        "id": "8cb4ig8ueDlO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCvMbuvI7GN6Er2c5+PY04",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}